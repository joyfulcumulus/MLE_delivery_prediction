{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b50484d2-bfa4-40b6-82cd-e8bf86187cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pprint\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DateType\n",
    "\n",
    "import utils.data_processing_bronze_table as bronze_processing\n",
    "import utils.data_processing_silver_table as silver_processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c91bb1-bcf0-4195-90f3-dc88806ebf8c",
   "metadata": {},
   "source": [
    "## set up pyspark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a822439-ea37-4662-a83e-1a4d48b1ab8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---starting job---\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/17 06:16:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n---starting job---\\n\\n')\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName(\"olist_bronze_processing\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to ERROR to hide warnings\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5cddd1-b87b-42dd-8158-8ecf9bd6b839",
   "metadata": {},
   "source": [
    "## Build Bronze Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17e56e7-671b-4582-a4a7-9d551e83a226",
   "metadata": {},
   "source": [
    "Important note: There is some discrepancy in where the datamart folder is created when the main.py script is run vs this Jupyter notebook is run.\n",
    "\n",
    "* This Jupyter notebook will create the datamart folder inside `scripts` folder and output the bronze tables there.\n",
    "* When you run the main.py script, the datamart folder will be created inside `app` folder (i.e. root) and output the bronze tables there.\n",
    "\n",
    "Need to have team meeting to resolve this\n",
    "\n",
    "I chose to run the main.py script, therefore subsequent code on Silver Tables built references the path from `app` folder to access the bronze tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99b6d22e-b746-4247-80a9-ecdd471b7dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze root directory: datamart/bronze\n"
     ]
    }
   ],
   "source": [
    "# Create bronze root directory\n",
    "bronze_root = \"datamart/bronze\"\n",
    "os.makedirs(bronze_root, exist_ok=True)\n",
    "print(f\"Bronze root directory: {bronze_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20351f49-7a7b-4556-94af-db3b92c37668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all Olist datasets\n",
    "print(\"\\nProcessing Olist datasets...\")\n",
    "bronze_processing.process_olist_customers_bronze(bronze_root, spark)\n",
    "bronze_processing.process_olist_geolocation_bronze(bronze_root, spark)\n",
    "bronze_processing.process_olist_order_items_bronze(bronze_root, spark)\n",
    "bronze_processing.process_olist_order_payments_bronze(bronze_root, spark)\n",
    "bronze_processing.process_olist_order_reviews_bronze(bronze_root, spark)\n",
    "bronze_processing.process_olist_products_bronze(bronze_root, spark)\n",
    "bronze_processing.process_olist_sellers_bronze(bronze_root, spark)\n",
    "bronze_processing.process_product_cat_translation_bronze(bronze_root, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10db75df-e10c-477e-9433-c107cf8963bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process orders with monthly partitioning\n",
    "bronze_processing.process_olist_orders_bronze(bronze_root, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e42811-8a18-4f1d-bf62-7108dc267e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------------+--------------------+--------------+\n",
      "|         customer_id|  customer_unique_id|customer_zip_code_prefix|       customer_city|customer_state|\n",
      "+--------------------+--------------------+------------------------+--------------------+--------------+\n",
      "|503840d4f2a1a7609...|ffc4233210eac4ec1...|                   14811|          araraquara|            SP|\n",
      "|52e73a5d0a1d4c56b...|b43530186123fb6d9...|                   62625|               missi|            CE|\n",
      "|16cb62869f9719571...|c3cc321141423ab8a...|                   55560|           barreiros|            PE|\n",
      "|4979ba0e6037e4b28...|80768413a59684f1e...|                   29307|cachoeiro de itap...|            ES|\n",
      "|11ec4bc0610184925...|bd836cf4fce7f808b...|                   22420|      rio de janeiro|            RJ|\n",
      "+--------------------+--------------------+------------------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect some output\n",
    "# I put the actual path due to the discrepancy in paths above. Will amend later\n",
    "df_bronze = spark.read.parquet(\"../datamart/bronze/customers/bronze_olist_customers.parquet\")\n",
    "df_bronze.show(5)\n",
    "\n",
    "# Can read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d767bf08-b027-43bd-a695-43b5217fd756",
   "metadata": {},
   "source": [
    "## Build Silver Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573d6071-2d84-491f-81d9-a2f0f692e526",
   "metadata": {},
   "source": [
    "Important note: There is some discrepancy in where the datamart folder is created when the main.py script is run vs this Jupyter notebook is run.\n",
    "\n",
    "* This Jupyter notebook will create the datamart folder inside `scripts` folder and output the silver tables there.\n",
    "* When you run the main.py script, the datamart folder will be created inside `app` folder (i.e. root) and output the silver tables there.\n",
    "\n",
    "Need to have team meeting to resolve this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd2b29d3-9ac3-4246-a52f-d67fadf64940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver root directory: datamart/silver\n"
     ]
    }
   ],
   "source": [
    "# Create silver root directory\n",
    "silver_root = \"datamart/silver\"\n",
    "os.makedirs(silver_root, exist_ok=True)\n",
    "print(f\"Silver root directory: {silver_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1012f779-1308-4a32-9e1e-e6bfc9289cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all required output directories\n",
    "\n",
    "# Create silver directory to save customer data\n",
    "silver_cust_directory = \"datamart/silver/customers/\"\n",
    "if not os.path.exists(silver_cust_directory):\n",
    "    os.makedirs(silver_cust_directory)\n",
    "\n",
    "# Create silver directory to save seller data\n",
    "silver_sell_directory = \"datamart/silver/sellers/\"\n",
    "if not os.path.exists(silver_sell_directory):\n",
    "    os.makedirs(silver_sell_directory)\n",
    "\n",
    "# Create silver directory to save geolocation data\n",
    "silver_geo_directory = \"datamart/silver/geolocation/\"\n",
    "if not os.path.exists(silver_geo_directory):\n",
    "    os.makedirs(silver_geo_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a4dee92-5661-46b9-85ad-e84d0448147f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing bronze tables...\n",
      "loaded from: ../datamart/bronze/customers/bronze_olist_customers.parquet row count: 99441\n",
      "Number of duplicated 'customer_id': 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: datamart/silver/customers/silver_olist_customers.parquet\n",
      "loaded from: ../datamart/bronze/sellers/bronze_olist_sellers.parquet row count: 3095\n",
      "Number of duplicated 'seller_id': 0\n",
      "saved to: datamart/silver/sellers/silver_olist_sellers.parquet\n",
      "loaded from: ../datamart/bronze/geolocation/bronze_olist_geolocation.parquet row count: 1000325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: datamart/silver/geolocation/silver_olist_geolocation.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[geolocation_zip_code_prefix: string, geolocation_lat: double, geolocation_lng: double]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process all bronze tables into silver\n",
    "print(\"\\nProcessing bronze tables...\")\n",
    "silver_processing.process_silver_olist_customers(\"../datamart/bronze/customers/\",silver_cust_directory, spark)\n",
    "silver_processing.process_silver_olist_sellers(\"../datamart/bronze/sellers/\",silver_sell_directory, spark)\n",
    "silver_processing.process_silver_olist_geolocation(\"../datamart/bronze/geolocation/\",silver_geo_directory, spark)\n",
    "# add more below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7735fc5a-0b84-4d4d-9a39-5fcf13b359aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process orders with monthly partitioning\n",
    "# add more below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9692dab1-3c00-427d-9abf-9319dadf04d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-------------------+-------------------+\n",
      "|geolocation_zip_code_prefix|    geolocation_lat|    geolocation_lng|\n",
      "+---------------------------+-------------------+-------------------+\n",
      "|                      49290|-11.274805005391439|-37.790795516967776|\n",
      "|                      49630|-10.605308055877686|-37.113027572631836|\n",
      "|                      55445|   -8.5616774559021|  -35.8295783996582|\n",
      "|                      57051| -9.655002400681779| -35.73440123893119|\n",
      "|                      57085| -9.558634171119103| -35.73914117079515|\n",
      "+---------------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect some output\n",
    "df_silver = spark.read.parquet(\"datamart/silver/geolocation/silver_olist_geolocation.parquet\")\n",
    "df_silver.show(5)\n",
    "\n",
    "# Can read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b00aa2-e29a-4965-866f-a1f593ab4c23",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Build Customer Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81fd1733-5963-4536-987b-528e045dba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create silver directory to save customer data\n",
    "silver_cust_directory = \"datamart/silver/customers/\"\n",
    "if not os.path.exists(silver_cust_directory):\n",
    "    os.makedirs(silver_cust_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8c4513d-86f0-4807-9cc4-eb1d3a694448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_silver_olist_customers(bronze_directory, silver_directory, spark):\n",
    "    \n",
    "    # connect to bronze table\n",
    "    partition_name = \"bronze_olist_customers.parquet\"\n",
    "    filepath = bronze_directory + partition_name\n",
    "    df = spark.read.parquet(filepath)\n",
    "    print('loaded from:', filepath, 'row count:', df.count())\n",
    "\n",
    "    # clean data: enforce schema / data type\n",
    "    # Dictionary specifying columns and their desired datatypes\n",
    "    column_type_map = {\n",
    "        \"customer_id\": StringType(),\n",
    "        \"customer_unique_id\": StringType(),\n",
    "        \"customer_zip_code_prefix\": StringType(),\n",
    "        \"customer_city\": StringType(),\n",
    "        \"customer_state\": StringType(),\n",
    "    }\n",
    "\n",
    "    for column, new_type in column_type_map.items():\n",
    "        df = df.withColumn(column, col(column).cast(new_type))\n",
    "\n",
    "    # Check customer_id duplicates (total rows - distinct ids)\n",
    "    total_rows = df.count()\n",
    "    distinct_rows = df.select(\"customer_id\").distinct().count()\n",
    "    duplicates_customer_id = total_rows - distinct_rows\n",
    "    print(f\"Number of duplicated 'customer_id': {duplicates_customer_id}\")\n",
    "\n",
    "    # Add missing leading zero\n",
    "    df = df.withColumn(\n",
    "        \"customer_zip_code_prefix\",\n",
    "        F.lpad(col(\"customer_zip_code_prefix\"), 5, \"0\")\n",
    "    )\n",
    "    \n",
    "    # save silver table - IRL connect to database to write\n",
    "    partition_name = \"silver_olist_customers.parquet\"\n",
    "    filepath = silver_directory + partition_name\n",
    "    df.write.mode(\"overwrite\").parquet(filepath)\n",
    "    print('saved to:', filepath)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14ca153f-67d1-412a-a729-7080f9bc5e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from: ../datamart/bronze/customers/bronze_olist_customers.parquet row count: 99441\n",
      "Number of duplicated 'customer_id': 0\n",
      "saved to: datamart/silver/customers/silver_olist_customers.parquet\n"
     ]
    }
   ],
   "source": [
    "# Run function manually to test\n",
    "# I inputted the bronze_directory manually (amend after our path discrepancies are resolved)\n",
    "df = process_silver_olist_customers(\"../datamart/bronze/customers/\",silver_cust_directory, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "825f8aef-d715-44d9-9f81-c8c7202432b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- customer_unique_id: string (nullable = true)\n",
      " |-- customer_zip_code_prefix: string (nullable = true)\n",
      " |-- customer_city: string (nullable = true)\n",
      " |-- customer_state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check schema enforced\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f823b48-1e8c-4b5c-a38e-0921dbae1c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|length|count|\n",
      "+------+-----+\n",
      "|     5|99441|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check missing leading zero padded\n",
    "df.groupBy(F.length(\"customer_zip_code_prefix\").alias(\"length\")).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2f0173-1008-4f1c-a776-f36daec1746b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Build Seller Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba12ae85-298f-454d-8a35-daddd3897a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create silver directory to save seller data\n",
    "silver_sell_directory = \"datamart/silver/sellers/\"\n",
    "if not os.path.exists(silver_sell_directory):\n",
    "    os.makedirs(silver_sell_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bea31b9-621e-4630-8b17-1106edf023a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_silver_olist_sellers(bronze_directory, silver_directory, spark):\n",
    "    \n",
    "    # connect to bronze table\n",
    "    partition_name = \"bronze_olist_sellers.parquet\"\n",
    "    filepath = bronze_directory + partition_name\n",
    "    df = spark.read.parquet(filepath)\n",
    "    print('loaded from:', filepath, 'row count:', df.count())\n",
    "\n",
    "    # clean data: enforce schema / data type\n",
    "    # Dictionary specifying columns and their desired datatypes\n",
    "    column_type_map = {\n",
    "        \"seller_id\": StringType(),\n",
    "        \"seller_zip_code_prefix\": StringType(),\n",
    "        \"seller_city\": StringType(),\n",
    "        \"seller_state\": StringType(),\n",
    "    }\n",
    "\n",
    "    for column, new_type in column_type_map.items():\n",
    "        df = df.withColumn(column, col(column).cast(new_type))\n",
    "\n",
    "    # Check seller_id duplicates (total rows - distinct ids)\n",
    "    total_rows = df.count()\n",
    "    distinct_rows = df.select(\"seller_id\").distinct().count()\n",
    "    duplicates_seller_id = total_rows - distinct_rows\n",
    "    print(f\"Number of duplicated 'seller_id': {duplicates_seller_id}\")\n",
    "\n",
    "    # Add missing leading zero\n",
    "    df = df.withColumn(\n",
    "        \"seller_zip_code_prefix\",\n",
    "        F.lpad(col(\"seller_zip_code_prefix\"), 5, \"0\")\n",
    "    )\n",
    "    \n",
    "    # save silver table - IRL connect to database to write\n",
    "    partition_name = \"silver_olist_sellers.parquet\"\n",
    "    filepath = silver_directory + partition_name\n",
    "    df.write.mode(\"overwrite\").parquet(filepath)\n",
    "    print('saved to:', filepath)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd3ef983-9e83-4970-85fd-d7358d2f3de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from: ../datamart/bronze/sellers/bronze_olist_sellers.parquet row count: 3095\n",
      "Number of duplicated 'seller_id': 0\n",
      "saved to: datamart/silver/sellers/silver_olist_sellers.parquet\n"
     ]
    }
   ],
   "source": [
    "# Run function manually to test\n",
    "# I inputted the bronze_directory manually (amend after our path discrepancies are resolved)\n",
    "df = process_silver_olist_sellers(\"../datamart/bronze/sellers/\",silver_sell_directory, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1468622b-78f2-4fd7-a0b9-6c2e4b5a6cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- seller_zip_code_prefix: string (nullable = true)\n",
      " |-- seller_city: string (nullable = true)\n",
      " |-- seller_state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check schema enforced\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "066674d2-760b-47fe-89ef-0be33600f1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|length|count|\n",
      "+------+-----+\n",
      "|     5| 3095|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check missing leading zero padded\n",
    "df.groupBy(F.length(\"seller_zip_code_prefix\").alias(\"length\")).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73580bff-eb08-412b-b91e-59ed8767181a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Build Geolocation Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8548b10-ed7f-41d6-b767-f23f4c771a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create silver directory to save geolocation data\n",
    "silver_geo_directory = \"datamart/silver/geolocation/\"\n",
    "if not os.path.exists(silver_geo_directory):\n",
    "    os.makedirs(silver_geo_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7437f8bf-83cd-475d-a948-4bffd41e2fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_silver_olist_geolocation(bronze_directory, silver_directory, spark):\n",
    "    \n",
    "    # connect to bronze table\n",
    "    partition_name = \"bronze_olist_geolocation.parquet\"\n",
    "    filepath = bronze_directory + partition_name\n",
    "    df = spark.read.parquet(filepath)\n",
    "    print('loaded from:', filepath, 'row count:', df.count())\n",
    "\n",
    "    # clean data: enforce schema / data type\n",
    "    # Dictionary specifying columns and their desired datatypes\n",
    "    column_type_map = {\n",
    "        \"geolocation_zip_code_prefix\": StringType(),\n",
    "        \"geolocation_lat\": FloatType(),\n",
    "        \"geolocation_lng\": FloatType(),\n",
    "        \"geolocation_city\": StringType(),\n",
    "        \"geolocation_state\": StringType(),\n",
    "    }\n",
    "\n",
    "    for column, new_type in column_type_map.items():\n",
    "        df = df.withColumn(column, col(column).cast(new_type))\n",
    "\n",
    "    # Add missing leading zero\n",
    "    df = df.withColumn(\n",
    "        \"geolocation_zip_code_prefix\",\n",
    "        F.lpad(col(\"geolocation_zip_code_prefix\"), 5, \"0\")\n",
    "    )\n",
    "\n",
    "    # Deduplicate zipcodes by just taking the centroid (mean of lat,lng)\n",
    "    df_dedupe = df.groupBy(\"geolocation_zip_code_prefix\").agg(\n",
    "        F.avg(\"geolocation_lat\").alias(\"geolocation_lat\"),\n",
    "        F.avg(\"geolocation_lng\").alias(\"geolocation_lng\")\n",
    "    )\n",
    "    \n",
    "    # save silver table - IRL connect to database to write\n",
    "    partition_name = \"silver_olist_geolocation.parquet\"\n",
    "    filepath = silver_directory + partition_name\n",
    "    df.write.mode(\"overwrite\").parquet(filepath)\n",
    "    print('saved to:', filepath)\n",
    "    \n",
    "    return df_dedupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "744285be-254a-482c-bbd4-f1b4dc73cef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from: ../datamart/bronze/geolocation/bronze_olist_geolocation.parquet row count: 1000325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: datamart/silver/geolocation/silver_olist_geolocation.parquet\n"
     ]
    }
   ],
   "source": [
    "# Run function manually to test\n",
    "# I inputted the bronze_directory manually (amend after our path discrepancies are resolved)\n",
    "df = process_silver_olist_geolocation(\"../datamart/bronze/geolocation/\",silver_geo_directory, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a145539-6949-49b3-bc73-f95e92a0191c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- geolocation_zip_code_prefix: string (nullable = true)\n",
      " |-- geolocation_lat: double (nullable = true)\n",
      " |-- geolocation_lng: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check schema enforced\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36a998b9-e0d1-46df-90f8-7ac1fc45901c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|length|count|\n",
      "+------+-----+\n",
      "|     5|19177|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check missing leading zero padded\n",
    "df.groupBy(F.length(\"geolocation_zip_code_prefix\").alias(\"length\")).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5857ffa-e899-4d38-a856-92d3a9c2a643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-----+\n",
      "|geolocation_zip_code_prefix|count|\n",
      "+---------------------------+-----+\n",
      "+---------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check every geolocation_zip_code_prefix only has 1 count. Group by prefix and count occurrences\n",
    "df.groupBy(\"geolocation_zip_code_prefix\") \\\n",
    "    .agg(F.count(\"*\").alias(\"count\")) \\\n",
    "    .filter(\"count > 1\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76752735-5f9b-42f2-8950-e1283efabee0",
   "metadata": {},
   "source": [
    "## Build Gold Table (Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9cb3e0-1fe4-48f0-9a46-0f300154ff8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c818d8e7-a22a-40ab-abdf-ace25c516543",
   "metadata": {},
   "source": [
    "## Inspect Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a7d963-2611-4896-be4a-7fbbd641f4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa5a6b17-8978-405a-8b04-6c6f4c7a3e52",
   "metadata": {},
   "source": [
    "## Build Gold Table (Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b200137d-ca20-42b9-a3df-0b662024a899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70eeb8a3-7737-4556-a85c-e844b47f6454",
   "metadata": {},
   "source": [
    "## Inspect Label Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc20833e-9653-4c4c-9ce0-0023cca719b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "974a0dd8-3602-42b8-b429-86cb7a2fe872",
   "metadata": {},
   "source": [
    "## Stop Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b97a33e6-9799-4a59-9588-40af6b036604",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# End spark session\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mspark\u001b[49m.stop()\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m---completed job---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# End spark session\n",
    "spark.stop()\n",
    "\n",
    "print('\\n\\n---completed job---\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
