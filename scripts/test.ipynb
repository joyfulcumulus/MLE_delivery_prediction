{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8fb66c-b902-4657-86b0-2da9069a7d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execute command: python gold_feature_store.py --startdate \"2016-09-04\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:19:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_09_04.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2016_09_04.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_09_04.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2016_09_04.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2016_09_04.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2016_09_04.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2016-09-04\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 0\n",
      "  items_df: 112650\n",
      "  logistic_df: 1\n",
      "  shipping_df: 1\n",
      "  history_df: 1\n",
      "  seller_perform_df: 0\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 0 rows\n",
      "After joining items_df: 0 rows\n",
      "After joining logistic_df: 0 rows\n",
      "After joining shipping_df: 0 rows\n",
      "After joining history_df: 0 rows\n",
      "After joining seller_perform_df: 0 rows\n",
      "After adding date columns: 0 rows\n",
      "After joining concentration_df: 0 rows\n",
      "After dropping unused columns: 0 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 0\n",
      "WARNING: Final DataFrame is empty! Check join conditions and data availability.\n",
      "+-------------+--------+------------+---------+-----------+-------------------+--------------+----------------+-------------+-----------+------------+-----------------+---------+----------+----------+-------------------+----------+--------------+-------------------+-----------+------+-------------+\n",
      "|snapshot_date|order_id|order_status|total_qty|total_price|total_freight_value|total_weight_g|total_volume_cm3|total_density|seller_city|seller_state|delivery_distance|same_city|same_state|is_weekend|act_days_to_deliver|avg_rating|avg_delay_rate|avg_processing_time|day_of_week|season|concentration|\n",
      "+-------------+--------+------------+---------+-----------+-------------------+--------------+----------------+-------------+-----------+------------+-----------------+---------+----------+----------+-------------------+----------+--------------+-------------------+-----------+------+-------------+\n",
      "+-------------+--------+------------+---------+-----------+-------------------+--------------+----------------+-------------+-----------+------------+-----------------+---------+----------+----------+-------------------+----------+--------------+-------------------+-----------+------+-------------+\n",
      "\n",
      "Number of rows in feature store: 0\n",
      "Gold feature tables built successfully from start date: 2016-09-04\n",
      "execute command: python gold_feature_store.py --startdate \"2016-09-05\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:19:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_09_05.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2016_09_05.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_09_05.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2016_09_05.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2016_09_05.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2016_09_05.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2016-09-05\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 0\n",
      "  items_df: 112650\n",
      "  logistic_df: 1\n",
      "  shipping_df: 1\n",
      "  history_df: 1\n",
      "  seller_perform_df: 0\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 0 rows\n",
      "After joining items_df: 0 rows\n",
      "After joining logistic_df: 0 rows\n",
      "After joining shipping_df: 0 rows\n",
      "After joining history_df: 0 rows\n",
      "After joining seller_perform_df: 0 rows\n",
      "After adding date columns: 0 rows\n",
      "After joining concentration_df: 0 rows\n",
      "After dropping unused columns: 0 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 0\n",
      "WARNING: Final DataFrame is empty! Check join conditions and data availability.\n",
      "+-------------+--------+------------+---------+-----------+-------------------+--------------+----------------+-------------+-----------+------------+-----------------+---------+----------+----------+-------------------+----------+--------------+-------------------+-----------+------+-------------+\n",
      "|snapshot_date|order_id|order_status|total_qty|total_price|total_freight_value|total_weight_g|total_volume_cm3|total_density|seller_city|seller_state|delivery_distance|same_city|same_state|is_weekend|act_days_to_deliver|avg_rating|avg_delay_rate|avg_processing_time|day_of_week|season|concentration|\n",
      "+-------------+--------+------------+---------+-----------+-------------------+--------------+----------------+-------------+-----------+------------+-----------------+---------+----------+----------+-------------------+----------+--------------+-------------------+-----------+------+-------------+\n",
      "+-------------+--------+------------+---------+-----------+-------------------+--------------+----------------+-------------+-----------+------------+-----------------+---------+----------+----------+-------------------+----------+--------------+-------------------+-----------+------+-------------+\n",
      "\n",
      "Number of rows in feature store: 0\n",
      "Gold feature tables built successfully from start date: 2016-09-05\n",
      "execute command: python gold_feature_store.py --startdate \"2016-09-06\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:19:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_09_06.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Available files in order_logistics: ['silver_olist_order_logistics_2016_09_04.parquet', 'silver_olist_order_logistics_2016_09_05.parquet', 'silver_olist_order_logistics_2016_09_13.parquet', 'silver_olist_order_logistics_2016_09_15.parquet', 'silver_olist_order_logistics_2016_10_02.parquet']...\n",
      "execute command: python gold_feature_store.py --startdate \"2016-09-07\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/app/scripts/gold_feature_store.py\", line 109, in <module>\n",
      "    logistic_df = read_silver_table('order_logistics', silver_directory, spark, date_str=snapshot_date_str)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/app/scripts/gold_feature_store.py\", line 59, in read_silver_table\n",
      "    raise FileNotFoundError(f\"No files found for table '{table}' with pattern '{pattern}' (date: '{date_str}')\")\n",
      "FileNotFoundError: No files found for table 'order_logistics' with pattern 'silver_olist_order_logistics_2016_09_06.parquet' (date: '2016-09-06')\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:19:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_09_07.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Available files in order_logistics: ['silver_olist_order_logistics_2016_09_04.parquet', 'silver_olist_order_logistics_2016_09_05.parquet', 'silver_olist_order_logistics_2016_09_13.parquet', 'silver_olist_order_logistics_2016_09_15.parquet', 'silver_olist_order_logistics_2016_10_02.parquet']...\n",
      "execute command: python gold_feature_store.py --startdate \"2016-09-08\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/app/scripts/gold_feature_store.py\", line 109, in <module>\n",
      "    logistic_df = read_silver_table('order_logistics', silver_directory, spark, date_str=snapshot_date_str)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/app/scripts/gold_feature_store.py\", line 59, in read_silver_table\n",
      "    raise FileNotFoundError(f\"No files found for table '{table}' with pattern '{pattern}' (date: '{date_str}')\")\n",
      "FileNotFoundError: No files found for table 'order_logistics' with pattern 'silver_olist_order_logistics_2016_09_07.parquet' (date: '2016-09-07')\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:20:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_09_08.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Available files in order_logistics: ['silver_olist_order_logistics_2016_09_04.parquet', 'silver_olist_order_logistics_2016_09_05.parquet', 'silver_olist_order_logistics_2016_09_13.parquet', 'silver_olist_order_logistics_2016_09_15.parquet', 'silver_olist_order_logistics_2016_10_02.parquet']...\n",
      "execute command: python gold_feature_store.py --startdate \"2016-09-09\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/app/scripts/gold_feature_store.py\", line 109, in <module>\n",
      "    logistic_df = read_silver_table('order_logistics', silver_directory, spark, date_str=snapshot_date_str)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/app/scripts/gold_feature_store.py\", line 59, in read_silver_table\n",
      "    raise FileNotFoundError(f\"No files found for table '{table}' with pattern '{pattern}' (date: '{date_str}')\")\n",
      "FileNotFoundError: No files found for table 'order_logistics' with pattern 'silver_olist_order_logistics_2016_09_08.parquet' (date: '2016-09-08')\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:20:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_09_09.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Available files in order_logistics: ['silver_olist_order_logistics_2016_09_04.parquet', 'silver_olist_order_logistics_2016_09_05.parquet', 'silver_olist_order_logistics_2016_09_13.parquet', 'silver_olist_order_logistics_2016_09_15.parquet', 'silver_olist_order_logistics_2016_10_02.parquet']...\n",
      "execute command: python gold_feature_store.py --startdate \"2016-09-10\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/app/scripts/gold_feature_store.py\", line 109, in <module>\n",
      "    logistic_df = read_silver_table('order_logistics', silver_directory, spark, date_str=snapshot_date_str)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/app/scripts/gold_feature_store.py\", line 59, in read_silver_table\n",
      "    raise FileNotFoundError(f\"No files found for table '{table}' with pattern '{pattern}' (date: '{date_str}')\")\n",
      "FileNotFoundError: No files found for table 'order_logistics' with pattern 'silver_olist_order_logistics_2016_09_09.parquet' (date: '2016-09-09')\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:20:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_09_10.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Available files in order_logistics: ['silver_olist_order_logistics_2016_09_04.parquet', 'silver_olist_order_logistics_2016_09_05.parquet', 'silver_olist_order_logistics_2016_09_13.parquet', 'silver_olist_order_logistics_2016_09_15.parquet', 'silver_olist_order_logistics_2016_10_02.parquet']...\n",
      "execute command: python gold_feature_store.py --startdate \"2016-09-11\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/app/scripts/gold_feature_store.py\", line 109, in <module>\n",
      "    logistic_df = read_silver_table('order_logistics', silver_directory, spark, date_str=snapshot_date_str)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/app/scripts/gold_feature_store.py\", line 59, in read_silver_table\n",
      "    raise FileNotFoundError(f\"No files found for table '{table}' with pattern '{pattern}' (date: '{date_str}')\")\n",
      "FileNotFoundError: No files found for table 'order_logistics' with pattern 'silver_olist_order_logistics_2016_09_10.parquet' (date: '2016-09-10')\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:20:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-09-11\n",
      "execute command: python gold_feature_store.py --startdate \"2016-09-12\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:20:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-09-12\n",
      "execute command: python gold_feature_store.py --startdate \"2016-09-13\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:20:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_09_13.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2016_09_13.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_09_13.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2016_09_13.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2016_09_13.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2016_09_13.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2016-09-13\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 0\n",
      "  items_df: 112650\n",
      "  logistic_df: 0\n",
      "  shipping_df: 0\n",
      "  history_df: 0\n",
      "  seller_perform_df: 0\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 0 rows\n",
      "After joining items_df: 0 rows\n",
      "After joining logistic_df: 0 rows\n",
      "After joining shipping_df: 0 rows\n",
      "After joining history_df: 0 rows\n",
      "After joining seller_perform_df: 0 rows\n",
      "After adding date columns: 0 rows\n",
      "After joining concentration_df: 0 rows\n",
      "After dropping unused columns: 0 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 0\n",
      "WARNING: Final DataFrame is empty! Check join conditions and data availability.\n",
      "+-------------+--------+------------+---------+-----------+-------------------+--------------+----------------+-------------+-----------+------------+-----------------+---------+----------+----------+-------------------+----------+--------------+-------------------+-----------+------+-------------+\n",
      "|snapshot_date|order_id|order_status|total_qty|total_price|total_freight_value|total_weight_g|total_volume_cm3|total_density|seller_city|seller_state|delivery_distance|same_city|same_state|is_weekend|act_days_to_deliver|avg_rating|avg_delay_rate|avg_processing_time|day_of_week|season|concentration|\n",
      "+-------------+--------+------------+---------+-----------+-------------------+--------------+----------------+-------------+-----------+------------+-----------------+---------+----------+----------+-------------------+----------+--------------+-------------------+-----------+------+-------------+\n",
      "+-------------+--------+------------+---------+-----------+-------------------+--------------+----------------+-------------+-----------+------------+-----------------+---------+----------+----------+-------------------+----------+--------------+-------------------+-----------+------+-------------+\n",
      "\n",
      "Number of rows in feature store: 0\n",
      "Gold feature tables built successfully from start date: 2016-09-13\n",
      "execute command: python gold_feature_store.py --startdate \"2016-09-14\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:20:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-09-14\n",
      "execute command: python gold_feature_store.py --startdate \"2016-09-15\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:20:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_09_15.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2016_09_15.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_09_15.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2016_09_15.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2016_09_15.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2016_09_15.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2016-09-15\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 1\n",
      "  items_df: 112650\n",
      "  logistic_df: 1\n",
      "  shipping_df: 1\n",
      "  history_df: 1\n",
      "  seller_perform_df: 1\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 1 rows\n",
      "After joining items_df: 1 rows\n",
      "After joining logistic_df: 1 rows\n",
      "After joining shipping_df: 1 rows\n",
      "After joining history_df: 1 rows\n",
      "After joining seller_perform_df: 1 rows\n",
      "After adding date columns: 1 rows\n",
      "After joining concentration_df: 1 rows\n",
      "After dropping unused columns: 1 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 1\n",
      "saved to: datamart/gold/feature_store/2016_09_15.parquet\n",
      "Feature gold table processing completed for snapshot date: 2016-09-15\n",
      "Number of rows in feature store: 1\n",
      "Gold feature tables built successfully from start date: 2016-09-15\n",
      "execute command: python gold_feature_store.py --startdate \"2016-09-16\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:20:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-09-16\n",
      "execute command: python gold_feature_store.py --startdate \"2016-09-17\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:20:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-09-17\n",
      "execute command: python gold_feature_store.py --startdate \"2016-09-18\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:20:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-09-18\n",
      "execute command: python gold_feature_store.py --startdate \"2016-09-19\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:20:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-09-19\n",
      "execute command: python gold_feature_store.py --startdate \"2016-09-20\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:20:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-09-20\n",
      "execute command: python gold_feature_store.py --startdate \"2016-09-21\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:21:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-09-21\n",
      "execute command: python gold_feature_store.py --startdate \"2016-09-22\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:21:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-09-22\n",
      "execute command: python gold_feature_store.py --startdate \"2016-09-23\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:21:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-09-23\n",
      "execute command: python gold_feature_store.py --startdate \"2016-09-24\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:21:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-09-24\n",
      "execute command: python gold_feature_store.py --startdate \"2016-09-25\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:21:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-09-25\n",
      "execute command: python gold_feature_store.py --startdate \"2016-09-26\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:21:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-09-26\n",
      "execute command: python gold_feature_store.py --startdate \"2016-09-27\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:21:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-09-27\n",
      "execute command: python gold_feature_store.py --startdate \"2016-09-28\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:21:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-09-28\n",
      "execute command: python gold_feature_store.py --startdate \"2016-09-29\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:21:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-09-29\n",
      "execute command: python gold_feature_store.py --startdate \"2016-09-30\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:21:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-09-30\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-01\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:21:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-10-01\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-02\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:21:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_10_02.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2016_10_02.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_10_02.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2016_10_02.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2016_10_02.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2016_10_02.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2016-10-02\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 1\n",
      "  items_df: 112650\n",
      "  logistic_df: 1\n",
      "  shipping_df: 1\n",
      "  history_df: 1\n",
      "  seller_perform_df: 1\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 1 rows\n",
      "After joining items_df: 1 rows\n",
      "After joining logistic_df: 1 rows\n",
      "After joining shipping_df: 1 rows\n",
      "After joining history_df: 1 rows\n",
      "After joining seller_perform_df: 1 rows\n",
      "After adding date columns: 1 rows\n",
      "After joining concentration_df: 1 rows\n",
      "After dropping unused columns: 1 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 1\n",
      "saved to: datamart/gold/feature_store/2016_10_02.parquet\n",
      "Feature gold table processing completed for snapshot date: 2016-10-02\n",
      "Number of rows in feature store: 1\n",
      "Gold feature tables built successfully from start date: 2016-10-02\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-03\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:21:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_10_03.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2016_10_03.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_10_03.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2016_10_03.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2016_10_03.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2016_10_03.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2016-10-03\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 8\n",
      "  items_df: 112650\n",
      "  logistic_df: 8\n",
      "  shipping_df: 8\n",
      "  history_df: 8\n",
      "  seller_perform_df: 7\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 8 rows\n",
      "After joining items_df: 8 rows\n",
      "After joining logistic_df: 8 rows\n",
      "After joining shipping_df: 8 rows\n",
      "After joining history_df: 8 rows\n",
      "After joining seller_perform_df: 8 rows\n",
      "After adding date columns: 8 rows\n",
      "After joining concentration_df: 8 rows\n",
      "After dropping unused columns: 8 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 8\n",
      "saved to: datamart/gold/feature_store/2016_10_03.parquet\n",
      "Feature gold table processing completed for snapshot date: 2016-10-03\n",
      "Number of rows in feature store: 8\n",
      "Gold feature tables built successfully from start date: 2016-10-03\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-04\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:21:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_10_04.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2016_10_04.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_10_04.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2016_10_04.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2016_10_04.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2016_10_04.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2016-10-04\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 60\n",
      "  items_df: 112650\n",
      "  logistic_df: 60\n",
      "  shipping_df: 60\n",
      "  history_df: 60\n",
      "  seller_perform_df: 39\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 60 rows\n",
      "After joining items_df: 60 rows\n",
      "After joining logistic_df: 60 rows\n",
      "After joining shipping_df: 60 rows\n",
      "After joining history_df: 60 rows\n",
      "After joining seller_perform_df: 60 rows\n",
      "After adding date columns: 60 rows\n",
      "After joining concentration_df: 60 rows\n",
      "After dropping unused columns: 60 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 60\n",
      "saved to: datamart/gold/feature_store/2016_10_04.parquet\n",
      "Feature gold table processing completed for snapshot date: 2016-10-04\n",
      "Number of rows in feature store: 60\n",
      "Gold feature tables built successfully from start date: 2016-10-04\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-05\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:22:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_10_05.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2016_10_05.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_10_05.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2016_10_05.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2016_10_05.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2016_10_05.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2016-10-05\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 42\n",
      "  items_df: 112650\n",
      "  logistic_df: 42\n",
      "  shipping_df: 42\n",
      "  history_df: 42\n",
      "  seller_perform_df: 34\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 42 rows\n",
      "After joining items_df: 42 rows\n",
      "After joining logistic_df: 42 rows\n",
      "After joining shipping_df: 42 rows\n",
      "After joining history_df: 42 rows\n",
      "After joining seller_perform_df: 42 rows\n",
      "After adding date columns: 42 rows\n",
      "After joining concentration_df: 42 rows\n",
      "After dropping unused columns: 42 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 42\n",
      "saved to: datamart/gold/feature_store/2016_10_05.parquet\n",
      "Feature gold table processing completed for snapshot date: 2016-10-05\n",
      "Number of rows in feature store: 42\n",
      "Gold feature tables built successfully from start date: 2016-10-05\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-06\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:22:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_10_06.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2016_10_06.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_10_06.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2016_10_06.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2016_10_06.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2016_10_06.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2016-10-06\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 49\n",
      "  items_df: 112650\n",
      "  logistic_df: 49\n",
      "  shipping_df: 49\n",
      "  history_df: 49\n",
      "  seller_perform_df: 40\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 49 rows\n",
      "After joining items_df: 49 rows\n",
      "After joining logistic_df: 49 rows\n",
      "After joining shipping_df: 49 rows\n",
      "After joining history_df: 49 rows\n",
      "After joining seller_perform_df: 49 rows\n",
      "After adding date columns: 49 rows\n",
      "After joining concentration_df: 49 rows\n",
      "After dropping unused columns: 49 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 49\n",
      "saved to: datamart/gold/feature_store/2016_10_06.parquet\n",
      "Feature gold table processing completed for snapshot date: 2016-10-06\n",
      "Number of rows in feature store: 49\n",
      "Gold feature tables built successfully from start date: 2016-10-06\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-07\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:22:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_10_07.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2016_10_07.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_10_07.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2016_10_07.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2016_10_07.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2016_10_07.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2016-10-07\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 45\n",
      "  items_df: 112650\n",
      "  logistic_df: 45\n",
      "  shipping_df: 45\n",
      "  history_df: 45\n",
      "  seller_perform_df: 36\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 45 rows\n",
      "After joining items_df: 45 rows\n",
      "After joining logistic_df: 45 rows\n",
      "After joining shipping_df: 45 rows\n",
      "After joining history_df: 45 rows\n",
      "After joining seller_perform_df: 45 rows\n",
      "After adding date columns: 45 rows\n",
      "After joining concentration_df: 45 rows\n",
      "After dropping unused columns: 45 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 45\n",
      "saved to: datamart/gold/feature_store/2016_10_07.parquet\n",
      "Feature gold table processing completed for snapshot date: 2016-10-07\n",
      "Number of rows in feature store: 45\n",
      "Gold feature tables built successfully from start date: 2016-10-07\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-08\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:22:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_10_08.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2016_10_08.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_10_08.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2016_10_08.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2016_10_08.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2016_10_08.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2016-10-08\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 40\n",
      "  items_df: 112650\n",
      "  logistic_df: 40\n",
      "  shipping_df: 40\n",
      "  history_df: 40\n",
      "  seller_perform_df: 32\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 40 rows\n",
      "After joining items_df: 40 rows\n",
      "After joining logistic_df: 40 rows\n",
      "After joining shipping_df: 40 rows\n",
      "After joining history_df: 40 rows\n",
      "After joining seller_perform_df: 40 rows\n",
      "After adding date columns: 40 rows\n",
      "After joining concentration_df: 40 rows\n",
      "After dropping unused columns: 40 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 40\n",
      "saved to: datamart/gold/feature_store/2016_10_08.parquet\n",
      "Feature gold table processing completed for snapshot date: 2016-10-08\n",
      "Number of rows in feature store: 40\n",
      "Gold feature tables built successfully from start date: 2016-10-08\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-09\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:23:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_10_09.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2016_10_09.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_10_09.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2016_10_09.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2016_10_09.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2016_10_09.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2016-10-09\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 26\n",
      "  items_df: 112650\n",
      "  logistic_df: 26\n",
      "  shipping_df: 26\n",
      "  history_df: 26\n",
      "  seller_perform_df: 25\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 26 rows\n",
      "After joining items_df: 26 rows\n",
      "After joining logistic_df: 26 rows\n",
      "After joining shipping_df: 26 rows\n",
      "After joining history_df: 26 rows\n",
      "After joining seller_perform_df: 26 rows\n",
      "After adding date columns: 26 rows\n",
      "After joining concentration_df: 26 rows\n",
      "After dropping unused columns: 26 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 26\n",
      "saved to: datamart/gold/feature_store/2016_10_09.parquet\n",
      "Feature gold table processing completed for snapshot date: 2016-10-09\n",
      "Number of rows in feature store: 26\n",
      "Gold feature tables built successfully from start date: 2016-10-09\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-10\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:23:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_10_10.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2016_10_10.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_10_10.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2016_10_10.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2016_10_10.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2016_10_10.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2016-10-10\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 37\n",
      "  items_df: 112650\n",
      "  logistic_df: 37\n",
      "  shipping_df: 37\n",
      "  history_df: 37\n",
      "  seller_perform_df: 31\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 37 rows\n",
      "After joining items_df: 37 rows\n",
      "After joining logistic_df: 37 rows\n",
      "After joining shipping_df: 37 rows\n",
      "After joining history_df: 37 rows\n",
      "After joining seller_perform_df: 37 rows\n",
      "After adding date columns: 37 rows\n",
      "After joining concentration_df: 37 rows\n",
      "After dropping unused columns: 37 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 37\n",
      "saved to: datamart/gold/feature_store/2016_10_10.parquet\n",
      "Feature gold table processing completed for snapshot date: 2016-10-10\n",
      "Number of rows in feature store: 37\n",
      "Gold feature tables built successfully from start date: 2016-10-10\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-11\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:23:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-10-11\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-12\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:23:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-10-12\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-13\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:23:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-10-13\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-14\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:23:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-10-14\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-15\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:23:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-10-15\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-16\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:23:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-10-16\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-17\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:23:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-10-17\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-18\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:24:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-10-18\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-19\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:24:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-10-19\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-20\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:24:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-10-20\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-21\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:24:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-10-21\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-22\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:24:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_10_22.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2016_10_22.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_10_22.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2016_10_22.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2016_10_22.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2016_10_22.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2016-10-22\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 0\n",
      "  items_df: 112650\n",
      "  logistic_df: 0\n",
      "  shipping_df: 0\n",
      "  history_df: 0\n",
      "  seller_perform_df: 0\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 0 rows\n",
      "After joining items_df: 0 rows\n",
      "After joining logistic_df: 0 rows\n",
      "After joining shipping_df: 0 rows\n",
      "After joining history_df: 0 rows\n",
      "After joining seller_perform_df: 0 rows\n",
      "After adding date columns: 0 rows\n",
      "After joining concentration_df: 0 rows\n",
      "After dropping unused columns: 0 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 0\n",
      "WARNING: Final DataFrame is empty! Check join conditions and data availability.\n",
      "+-------------+--------+------------+---------+-----------+-------------------+--------------+----------------+-------------+-----------+------------+-----------------+---------+----------+----------+-------------------+----------+--------------+-------------------+-----------+------+-------------+\n",
      "|snapshot_date|order_id|order_status|total_qty|total_price|total_freight_value|total_weight_g|total_volume_cm3|total_density|seller_city|seller_state|delivery_distance|same_city|same_state|is_weekend|act_days_to_deliver|avg_rating|avg_delay_rate|avg_processing_time|day_of_week|season|concentration|\n",
      "+-------------+--------+------------+---------+-----------+-------------------+--------------+----------------+-------------+-----------+------------+-----------------+---------+----------+----------+-------------------+----------+--------------+-------------------+-----------+------+-------------+\n",
      "+-------------+--------+------------+---------+-----------+-------------------+--------------+----------------+-------------+-----------+------------+-----------------+---------+----------+----------+-------------------+----------+--------------+-------------------+-----------+------+-------------+\n",
      "\n",
      "Number of rows in feature store: 0\n",
      "Gold feature tables built successfully from start date: 2016-10-22\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-23\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:24:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-10-23\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-24\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:24:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-10-24\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-25\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:24:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-10-25\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-26\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:24:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-10-26\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-27\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:24:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-10-27\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-28\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:24:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-10-28\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-29\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:24:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-10-29\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-30\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:24:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-10-30\n",
      "execute command: python gold_feature_store.py --startdate \"2016-10-31\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:24:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-10-31\n",
      "execute command: python gold_feature_store.py --startdate \"2016-11-01\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:24:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-11-01\n",
      "execute command: python gold_feature_store.py --startdate \"2016-11-02\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:24:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-11-02\n",
      "execute command: python gold_feature_store.py --startdate \"2016-11-03\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:24:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-11-03\n",
      "execute command: python gold_feature_store.py --startdate \"2016-11-04\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:25:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-11-04\n",
      "execute command: python gold_feature_store.py --startdate \"2016-11-05\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:25:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-11-05\n",
      "execute command: python gold_feature_store.py --startdate \"2016-11-06\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:25:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-11-06\n",
      "execute command: python gold_feature_store.py --startdate \"2016-11-07\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:25:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-11-07\n",
      "execute command: python gold_feature_store.py --startdate \"2016-11-08\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:25:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-11-08\n",
      "execute command: python gold_feature_store.py --startdate \"2016-11-09\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:25:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-11-09\n",
      "execute command: python gold_feature_store.py --startdate \"2016-11-10\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:25:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-11-10\n",
      "execute command: python gold_feature_store.py --startdate \"2016-11-11\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:25:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-11-11\n",
      "execute command: python gold_feature_store.py --startdate \"2016-11-12\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:25:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-11-12\n",
      "execute command: python gold_feature_store.py --startdate \"2016-11-13\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:25:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-11-13\n",
      "execute command: python gold_feature_store.py --startdate \"2016-11-14\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:25:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-11-14\n",
      "execute command: python gold_feature_store.py --startdate \"2016-11-15\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:25:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-11-15\n",
      "execute command: python gold_feature_store.py --startdate \"2016-11-16\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:25:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-11-16\n",
      "execute command: python gold_feature_store.py --startdate \"2016-11-17\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:25:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-11-17\n",
      "execute command: python gold_feature_store.py --startdate \"2016-11-18\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:25:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-11-18\n",
      "execute command: python gold_feature_store.py --startdate \"2016-11-19\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:25:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-11-19\n",
      "execute command: python gold_feature_store.py --startdate \"2016-11-20\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:25:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-11-20\n",
      "execute command: python gold_feature_store.py --startdate \"2016-11-21\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:25:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-11-21\n",
      "execute command: python gold_feature_store.py --startdate \"2016-11-22\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:25:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-11-22\n",
      "execute command: python gold_feature_store.py --startdate \"2016-11-23\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:25:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-11-23\n",
      "execute command: python gold_feature_store.py --startdate \"2016-11-24\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:25:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-11-24\n",
      "execute command: python gold_feature_store.py --startdate \"2016-11-25\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:25:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-11-25\n",
      "execute command: python gold_feature_store.py --startdate \"2016-11-26\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:26:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-11-26\n",
      "execute command: python gold_feature_store.py --startdate \"2016-11-27\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:26:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-11-27\n",
      "execute command: python gold_feature_store.py --startdate \"2016-11-28\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:26:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-11-28\n",
      "execute command: python gold_feature_store.py --startdate \"2016-11-29\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:26:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-11-29\n",
      "execute command: python gold_feature_store.py --startdate \"2016-11-30\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:26:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-11-30\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-01\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:26:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-12-01\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-02\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:26:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-12-02\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-03\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:26:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-12-03\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-04\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:26:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-12-04\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-05\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:26:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-12-05\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-06\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:26:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-12-06\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-07\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:26:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-12-07\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-08\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:26:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-12-08\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-09\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:26:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-12-09\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-10\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:26:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-12-10\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-11\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:26:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-12-11\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-12\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:26:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-12-12\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-13\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:26:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-12-13\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-14\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:26:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-12-14\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-15\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:26:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-12-15\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-16\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:26:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-12-16\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-17\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:26:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-12-17\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-18\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:27:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-12-18\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-19\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:27:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-12-19\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-20\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:27:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-12-20\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-21\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:27:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-12-21\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-22\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:27:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-12-22\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-23\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:27:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_12_23.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2016_12_23.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2016_12_23.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2016_12_23.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2016_12_23.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2016_12_23.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2016-12-23\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 1\n",
      "  items_df: 112650\n",
      "  logistic_df: 1\n",
      "  shipping_df: 1\n",
      "  history_df: 1\n",
      "  seller_perform_df: 1\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 1 rows\n",
      "After joining items_df: 1 rows\n",
      "After joining logistic_df: 1 rows\n",
      "After joining shipping_df: 1 rows\n",
      "After joining history_df: 1 rows\n",
      "After joining seller_perform_df: 1 rows\n",
      "After adding date columns: 1 rows\n",
      "After joining concentration_df: 1 rows\n",
      "After dropping unused columns: 1 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 1\n",
      "saved to: datamart/gold/feature_store/2016_12_23.parquet\n",
      "Feature gold table processing completed for snapshot date: 2016-12-23\n",
      "Number of rows in feature store: 1\n",
      "Gold feature tables built successfully from start date: 2016-12-23\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-24\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:27:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-12-24\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-25\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:27:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-12-25\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-26\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:27:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-12-26\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-27\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:27:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-12-27\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-28\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:27:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-12-28\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-29\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:27:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-12-29\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-30\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:27:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-12-30\n",
      "execute command: python gold_feature_store.py --startdate \"2016-12-31\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:27:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2016-12-31\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-01\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:27:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2017-01-01\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-02\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:27:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2017-01-02\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-03\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:28:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2017-01-03\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-04\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:28:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Available files in orders: ['silver_olist_orders_2016_09_04.parquet', 'silver_olist_orders_2016_09_05.parquet', 'silver_olist_orders_2016_09_06.parquet', 'silver_olist_orders_2016_09_07.parquet', 'silver_olist_orders_2016_09_08.parquet']...\n",
      "Gold feature tables built successfully from start date: 2017-01-04\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-05\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:28:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_05.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_01_05.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_05.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_01_05.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_01_05.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_01_05.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-01-05\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 32\n",
      "  items_df: 112650\n",
      "  logistic_df: 32\n",
      "  shipping_df: 32\n",
      "  history_df: 32\n",
      "  seller_perform_df: 3\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 32 rows\n",
      "After joining items_df: 32 rows\n",
      "After joining logistic_df: 32 rows\n",
      "After joining shipping_df: 32 rows\n",
      "After joining history_df: 32 rows\n",
      "After joining seller_perform_df: 32 rows\n",
      "After adding date columns: 32 rows\n",
      "After joining concentration_df: 32 rows\n",
      "After dropping unused columns: 32 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 32\n",
      "saved to: datamart/gold/feature_store/2017_01_05.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-01-05\n",
      "Number of rows in feature store: 32\n",
      "Gold feature tables built successfully from start date: 2017-01-05\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-06\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:28:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_06.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_01_06.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_06.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_01_06.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_01_06.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_01_06.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-01-06\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 4\n",
      "  items_df: 112650\n",
      "  logistic_df: 4\n",
      "  shipping_df: 4\n",
      "  history_df: 4\n",
      "  seller_perform_df: 4\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 4 rows\n",
      "After joining items_df: 4 rows\n",
      "After joining logistic_df: 4 rows\n",
      "After joining shipping_df: 4 rows\n",
      "After joining history_df: 4 rows\n",
      "After joining seller_perform_df: 4 rows\n",
      "After adding date columns: 4 rows\n",
      "After joining concentration_df: 4 rows\n",
      "After dropping unused columns: 4 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 4\n",
      "saved to: datamart/gold/feature_store/2017_01_06.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-01-06\n",
      "Number of rows in feature store: 4\n",
      "Gold feature tables built successfully from start date: 2017-01-06\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-07\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:28:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_07.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_01_07.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_07.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_01_07.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_01_07.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_01_07.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-01-07\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 4\n",
      "  items_df: 112650\n",
      "  logistic_df: 4\n",
      "  shipping_df: 4\n",
      "  history_df: 4\n",
      "  seller_perform_df: 4\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 4 rows\n",
      "After joining items_df: 4 rows\n",
      "After joining logistic_df: 4 rows\n",
      "After joining shipping_df: 4 rows\n",
      "After joining history_df: 4 rows\n",
      "After joining seller_perform_df: 4 rows\n",
      "After adding date columns: 4 rows\n",
      "After joining concentration_df: 4 rows\n",
      "After dropping unused columns: 4 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 4\n",
      "saved to: datamart/gold/feature_store/2017_01_07.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-01-07\n",
      "Number of rows in feature store: 4\n",
      "Gold feature tables built successfully from start date: 2017-01-07\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-08\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:28:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_08.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_01_08.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_08.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_01_08.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_01_08.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_01_08.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-01-08\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 6\n",
      "  items_df: 112650\n",
      "  logistic_df: 6\n",
      "  shipping_df: 6\n",
      "  history_df: 6\n",
      "  seller_perform_df: 5\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 6 rows\n",
      "After joining items_df: 6 rows\n",
      "After joining logistic_df: 6 rows\n",
      "After joining shipping_df: 6 rows\n",
      "After joining history_df: 6 rows\n",
      "After joining seller_perform_df: 6 rows\n",
      "After adding date columns: 6 rows\n",
      "After joining concentration_df: 6 rows\n",
      "After dropping unused columns: 6 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 6\n",
      "saved to: datamart/gold/feature_store/2017_01_08.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-01-08\n",
      "Number of rows in feature store: 6\n",
      "Gold feature tables built successfully from start date: 2017-01-08\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-09\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:29:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_09.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_01_09.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_09.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_01_09.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_01_09.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_01_09.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-01-09\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 5\n",
      "  items_df: 112650\n",
      "  logistic_df: 5\n",
      "  shipping_df: 5\n",
      "  history_df: 5\n",
      "  seller_perform_df: 5\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 5 rows\n",
      "After joining items_df: 5 rows\n",
      "After joining logistic_df: 5 rows\n",
      "After joining shipping_df: 5 rows\n",
      "After joining history_df: 5 rows\n",
      "After joining seller_perform_df: 5 rows\n",
      "After adding date columns: 5 rows\n",
      "After joining concentration_df: 5 rows\n",
      "After dropping unused columns: 5 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 5\n",
      "saved to: datamart/gold/feature_store/2017_01_09.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-01-09\n",
      "Number of rows in feature store: 5\n",
      "Gold feature tables built successfully from start date: 2017-01-09\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-10\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:29:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_10.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_01_10.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_10.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_01_10.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_01_10.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_01_10.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-01-10\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 6\n",
      "  items_df: 112650\n",
      "  logistic_df: 6\n",
      "  shipping_df: 6\n",
      "  history_df: 6\n",
      "  seller_perform_df: 5\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 6 rows\n",
      "After joining items_df: 6 rows\n",
      "After joining logistic_df: 6 rows\n",
      "After joining shipping_df: 6 rows\n",
      "After joining history_df: 6 rows\n",
      "After joining seller_perform_df: 6 rows\n",
      "After adding date columns: 6 rows\n",
      "After joining concentration_df: 6 rows\n",
      "After dropping unused columns: 6 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 6\n",
      "saved to: datamart/gold/feature_store/2017_01_10.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-01-10\n",
      "Number of rows in feature store: 6\n",
      "Gold feature tables built successfully from start date: 2017-01-10\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-11\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:29:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_11.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_01_11.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_11.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_01_11.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_01_11.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_01_11.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-01-11\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 12\n",
      "  items_df: 112650\n",
      "  logistic_df: 12\n",
      "  shipping_df: 12\n",
      "  history_df: 12\n",
      "  seller_perform_df: 10\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 12 rows\n",
      "After joining items_df: 12 rows\n",
      "After joining logistic_df: 12 rows\n",
      "After joining shipping_df: 12 rows\n",
      "After joining history_df: 12 rows\n",
      "After joining seller_perform_df: 12 rows\n",
      "After adding date columns: 12 rows\n",
      "After joining concentration_df: 12 rows\n",
      "After dropping unused columns: 12 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 12\n",
      "saved to: datamart/gold/feature_store/2017_01_11.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-01-11\n",
      "Number of rows in feature store: 12\n",
      "Gold feature tables built successfully from start date: 2017-01-11\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-12\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:30:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_12.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_01_12.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_12.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_01_12.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_01_12.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_01_12.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-01-12\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 13\n",
      "  items_df: 112650\n",
      "  logistic_df: 13\n",
      "  shipping_df: 13\n",
      "  history_df: 13\n",
      "  seller_perform_df: 10\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 13 rows\n",
      "After joining items_df: 13 rows\n",
      "After joining logistic_df: 13 rows\n",
      "After joining shipping_df: 13 rows\n",
      "After joining history_df: 13 rows\n",
      "After joining seller_perform_df: 13 rows\n",
      "After adding date columns: 13 rows\n",
      "After joining concentration_df: 13 rows\n",
      "After dropping unused columns: 13 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 13\n",
      "saved to: datamart/gold/feature_store/2017_01_12.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-01-12\n",
      "Number of rows in feature store: 13\n",
      "Gold feature tables built successfully from start date: 2017-01-12\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-13\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:30:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_13.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_01_13.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_13.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_01_13.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_01_13.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_01_13.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-01-13\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 11\n",
      "  items_df: 112650\n",
      "  logistic_df: 11\n",
      "  shipping_df: 11\n",
      "  history_df: 11\n",
      "  seller_perform_df: 8\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 11 rows\n",
      "After joining items_df: 11 rows\n",
      "After joining logistic_df: 11 rows\n",
      "After joining shipping_df: 11 rows\n",
      "After joining history_df: 11 rows\n",
      "After joining seller_perform_df: 11 rows\n",
      "After adding date columns: 11 rows\n",
      "After joining concentration_df: 11 rows\n",
      "After dropping unused columns: 11 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 11\n",
      "saved to: datamart/gold/feature_store/2017_01_13.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-01-13\n",
      "Number of rows in feature store: 11\n",
      "Gold feature tables built successfully from start date: 2017-01-13\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-14\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:30:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_14.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_01_14.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_14.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_01_14.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_01_14.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_01_14.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-01-14\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 18\n",
      "  items_df: 112650\n",
      "  logistic_df: 18\n",
      "  shipping_df: 18\n",
      "  history_df: 18\n",
      "  seller_perform_df: 10\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 18 rows\n",
      "After joining items_df: 18 rows\n",
      "After joining logistic_df: 18 rows\n",
      "After joining shipping_df: 18 rows\n",
      "After joining history_df: 18 rows\n",
      "After joining seller_perform_df: 18 rows\n",
      "After adding date columns: 18 rows\n",
      "After joining concentration_df: 18 rows\n",
      "After dropping unused columns: 18 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 18\n",
      "saved to: datamart/gold/feature_store/2017_01_14.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-01-14\n",
      "Number of rows in feature store: 18\n",
      "Gold feature tables built successfully from start date: 2017-01-14\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-15\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:30:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_15.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_01_15.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_15.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_01_15.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_01_15.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_01_15.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-01-15\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 14\n",
      "  items_df: 112650\n",
      "  logistic_df: 14\n",
      "  shipping_df: 14\n",
      "  history_df: 14\n",
      "  seller_perform_df: 10\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 14 rows\n",
      "After joining items_df: 14 rows\n",
      "After joining logistic_df: 14 rows\n",
      "After joining shipping_df: 14 rows\n",
      "After joining history_df: 14 rows\n",
      "After joining seller_perform_df: 14 rows\n",
      "After adding date columns: 14 rows\n",
      "After joining concentration_df: 14 rows\n",
      "After dropping unused columns: 14 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 14\n",
      "saved to: datamart/gold/feature_store/2017_01_15.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-01-15\n",
      "Number of rows in feature store: 14\n",
      "Gold feature tables built successfully from start date: 2017-01-15\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-16\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:31:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_16.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_01_16.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_16.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_01_16.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_01_16.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_01_16.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-01-16\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 19\n",
      "  items_df: 112650\n",
      "  logistic_df: 19\n",
      "  shipping_df: 19\n",
      "  history_df: 19\n",
      "  seller_perform_df: 14\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 19 rows\n",
      "After joining items_df: 19 rows\n",
      "After joining logistic_df: 19 rows\n",
      "After joining shipping_df: 19 rows\n",
      "After joining history_df: 19 rows\n",
      "After joining seller_perform_df: 19 rows\n",
      "After adding date columns: 19 rows\n",
      "After joining concentration_df: 19 rows\n",
      "After dropping unused columns: 19 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 19\n",
      "saved to: datamart/gold/feature_store/2017_01_16.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-01-16\n",
      "Number of rows in feature store: 19\n",
      "Gold feature tables built successfully from start date: 2017-01-16\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-17\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:31:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_17.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_01_17.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_17.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_01_17.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_01_17.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_01_17.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-01-17\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 32\n",
      "  items_df: 112650\n",
      "  logistic_df: 32\n",
      "  shipping_df: 32\n",
      "  history_df: 32\n",
      "  seller_perform_df: 25\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 32 rows\n",
      "After joining items_df: 32 rows\n",
      "After joining logistic_df: 32 rows\n",
      "After joining shipping_df: 32 rows\n",
      "After joining history_df: 32 rows\n",
      "After joining seller_perform_df: 32 rows\n",
      "After adding date columns: 32 rows\n",
      "After joining concentration_df: 32 rows\n",
      "After dropping unused columns: 32 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 32\n",
      "saved to: datamart/gold/feature_store/2017_01_17.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-01-17\n",
      "Number of rows in feature store: 32\n",
      "Gold feature tables built successfully from start date: 2017-01-17\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-18\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:31:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_18.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_01_18.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_18.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_01_18.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_01_18.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_01_18.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-01-18\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 33\n",
      "  items_df: 112650\n",
      "  logistic_df: 33\n",
      "  shipping_df: 33\n",
      "  history_df: 33\n",
      "  seller_perform_df: 30\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 33 rows\n",
      "After joining items_df: 33 rows\n",
      "After joining logistic_df: 33 rows\n",
      "After joining shipping_df: 33 rows\n",
      "After joining history_df: 33 rows\n",
      "After joining seller_perform_df: 33 rows\n",
      "After adding date columns: 33 rows\n",
      "After joining concentration_df: 33 rows\n",
      "After dropping unused columns: 33 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 33\n",
      "saved to: datamart/gold/feature_store/2017_01_18.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-01-18\n",
      "Number of rows in feature store: 33\n",
      "Gold feature tables built successfully from start date: 2017-01-18\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-19\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:31:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_19.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_01_19.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_19.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_01_19.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_01_19.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_01_19.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-01-19\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 29\n",
      "  items_df: 112650\n",
      "  logistic_df: 29\n",
      "  shipping_df: 29\n",
      "  history_df: 29\n",
      "  seller_perform_df: 23\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 29 rows\n",
      "After joining items_df: 29 rows\n",
      "After joining logistic_df: 29 rows\n",
      "After joining shipping_df: 29 rows\n",
      "After joining history_df: 29 rows\n",
      "After joining seller_perform_df: 29 rows\n",
      "After adding date columns: 29 rows\n",
      "After joining concentration_df: 29 rows\n",
      "After dropping unused columns: 29 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 29\n",
      "saved to: datamart/gold/feature_store/2017_01_19.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-01-19\n",
      "Number of rows in feature store: 29\n",
      "Gold feature tables built successfully from start date: 2017-01-19\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-20\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:32:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_20.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_01_20.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_20.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_01_20.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_01_20.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_01_20.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-01-20\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 28\n",
      "  items_df: 112650\n",
      "  logistic_df: 28\n",
      "  shipping_df: 28\n",
      "  history_df: 28\n",
      "  seller_perform_df: 23\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 28 rows\n",
      "After joining items_df: 28 rows\n",
      "After joining logistic_df: 28 rows\n",
      "After joining shipping_df: 28 rows\n",
      "After joining history_df: 28 rows\n",
      "After joining seller_perform_df: 28 rows\n",
      "After adding date columns: 28 rows\n",
      "After joining concentration_df: 28 rows\n",
      "After dropping unused columns: 28 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 28\n",
      "saved to: datamart/gold/feature_store/2017_01_20.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-01-20\n",
      "Number of rows in feature store: 28\n",
      "Gold feature tables built successfully from start date: 2017-01-20\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-21\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:32:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_21.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_01_21.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_21.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_01_21.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_01_21.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_01_21.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-01-21\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 24\n",
      "  items_df: 112650\n",
      "  logistic_df: 24\n",
      "  shipping_df: 24\n",
      "  history_df: 24\n",
      "  seller_perform_df: 18\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 24 rows\n",
      "After joining items_df: 24 rows\n",
      "After joining logistic_df: 24 rows\n",
      "After joining shipping_df: 24 rows\n",
      "After joining history_df: 24 rows\n",
      "After joining seller_perform_df: 24 rows\n",
      "After adding date columns: 24 rows\n",
      "After joining concentration_df: 24 rows\n",
      "After dropping unused columns: 24 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 24\n",
      "saved to: datamart/gold/feature_store/2017_01_21.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-01-21\n",
      "Number of rows in feature store: 24\n",
      "Gold feature tables built successfully from start date: 2017-01-21\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-22\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:32:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_22.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_01_22.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_22.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_01_22.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_01_22.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_01_22.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-01-22\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 30\n",
      "  items_df: 112650\n",
      "  logistic_df: 30\n",
      "  shipping_df: 30\n",
      "  history_df: 30\n",
      "  seller_perform_df: 21\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 30 rows\n",
      "After joining items_df: 30 rows\n",
      "After joining logistic_df: 30 rows\n",
      "After joining shipping_df: 30 rows\n",
      "After joining history_df: 30 rows\n",
      "After joining seller_perform_df: 30 rows\n",
      "After adding date columns: 30 rows\n",
      "After joining concentration_df: 30 rows\n",
      "After dropping unused columns: 30 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 30\n",
      "saved to: datamart/gold/feature_store/2017_01_22.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-01-22\n",
      "Number of rows in feature store: 30\n",
      "Gold feature tables built successfully from start date: 2017-01-22\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-23\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:32:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_23.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_01_23.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_23.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_01_23.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_01_23.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_01_23.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-01-23\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 39\n",
      "  items_df: 112650\n",
      "  logistic_df: 39\n",
      "  shipping_df: 39\n",
      "  history_df: 39\n",
      "  seller_perform_df: 34\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 39 rows\n",
      "After joining items_df: 39 rows\n",
      "After joining logistic_df: 39 rows\n",
      "After joining shipping_df: 39 rows\n",
      "After joining history_df: 39 rows\n",
      "After joining seller_perform_df: 39 rows\n",
      "After adding date columns: 39 rows\n",
      "After joining concentration_df: 39 rows\n",
      "After dropping unused columns: 39 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 39\n",
      "saved to: datamart/gold/feature_store/2017_01_23.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-01-23\n",
      "Number of rows in feature store: 39\n",
      "Gold feature tables built successfully from start date: 2017-01-23\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-24\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:33:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_24.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_01_24.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_24.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_01_24.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_01_24.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_01_24.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-01-24\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 40\n",
      "  items_df: 112650\n",
      "  logistic_df: 40\n",
      "  shipping_df: 40\n",
      "  history_df: 40\n",
      "  seller_perform_df: 34\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 40 rows\n",
      "After joining items_df: 40 rows\n",
      "After joining logistic_df: 40 rows\n",
      "After joining shipping_df: 40 rows\n",
      "After joining history_df: 40 rows\n",
      "After joining seller_perform_df: 40 rows\n",
      "After adding date columns: 40 rows\n",
      "After joining concentration_df: 40 rows\n",
      "After dropping unused columns: 40 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 40\n",
      "saved to: datamart/gold/feature_store/2017_01_24.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-01-24\n",
      "Number of rows in feature store: 40\n",
      "Gold feature tables built successfully from start date: 2017-01-24\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-25\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:33:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_25.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_01_25.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_25.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_01_25.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_01_25.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_01_25.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-01-25\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 62\n",
      "  items_df: 112650\n",
      "  logistic_df: 62\n",
      "  shipping_df: 62\n",
      "  history_df: 62\n",
      "  seller_perform_df: 45\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 62 rows\n",
      "After joining items_df: 62 rows\n",
      "After joining logistic_df: 62 rows\n",
      "After joining shipping_df: 62 rows\n",
      "After joining history_df: 62 rows\n",
      "After joining seller_perform_df: 62 rows\n",
      "After adding date columns: 62 rows\n",
      "After joining concentration_df: 62 rows\n",
      "After dropping unused columns: 62 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 62\n",
      "saved to: datamart/gold/feature_store/2017_01_25.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-01-25\n",
      "Number of rows in feature store: 62\n",
      "Gold feature tables built successfully from start date: 2017-01-25\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-26\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:33:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_26.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_01_26.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_26.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_01_26.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_01_26.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_01_26.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-01-26\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 85\n",
      "  items_df: 112650\n",
      "  logistic_df: 85\n",
      "  shipping_df: 85\n",
      "  history_df: 85\n",
      "  seller_perform_df: 65\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 85 rows\n",
      "After joining items_df: 85 rows\n",
      "After joining logistic_df: 85 rows\n",
      "After joining shipping_df: 85 rows\n",
      "After joining history_df: 85 rows\n",
      "After joining seller_perform_df: 85 rows\n",
      "After adding date columns: 85 rows\n",
      "After joining concentration_df: 85 rows\n",
      "After dropping unused columns: 85 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 85\n",
      "saved to: datamart/gold/feature_store/2017_01_26.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-01-26\n",
      "Number of rows in feature store: 85\n",
      "Gold feature tables built successfully from start date: 2017-01-26\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-27\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:34:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_27.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_01_27.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_27.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_01_27.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_01_27.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_01_27.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-01-27\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 61\n",
      "  items_df: 112650\n",
      "  logistic_df: 61\n",
      "  shipping_df: 61\n",
      "  history_df: 61\n",
      "  seller_perform_df: 51\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 61 rows\n",
      "After joining items_df: 61 rows\n",
      "After joining logistic_df: 61 rows\n",
      "After joining shipping_df: 61 rows\n",
      "After joining history_df: 61 rows\n",
      "After joining seller_perform_df: 61 rows\n",
      "After adding date columns: 61 rows\n",
      "After joining concentration_df: 61 rows\n",
      "After dropping unused columns: 61 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 61\n",
      "saved to: datamart/gold/feature_store/2017_01_27.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-01-27\n",
      "Number of rows in feature store: 61\n",
      "Gold feature tables built successfully from start date: 2017-01-27\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-28\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:34:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_28.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_01_28.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_28.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_01_28.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_01_28.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_01_28.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-01-28\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 29\n",
      "  items_df: 112650\n",
      "  logistic_df: 29\n",
      "  shipping_df: 29\n",
      "  history_df: 29\n",
      "  seller_perform_df: 27\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 29 rows\n",
      "After joining items_df: 29 rows\n",
      "After joining logistic_df: 29 rows\n",
      "After joining shipping_df: 29 rows\n",
      "After joining history_df: 29 rows\n",
      "After joining seller_perform_df: 29 rows\n",
      "After adding date columns: 29 rows\n",
      "After joining concentration_df: 29 rows\n",
      "After dropping unused columns: 29 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 29\n",
      "saved to: datamart/gold/feature_store/2017_01_28.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-01-28\n",
      "Number of rows in feature store: 29\n",
      "Gold feature tables built successfully from start date: 2017-01-28\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-29\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:34:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_29.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_01_29.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_29.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_01_29.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_01_29.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_01_29.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-01-29\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 35\n",
      "  items_df: 112650\n",
      "  logistic_df: 35\n",
      "  shipping_df: 35\n",
      "  history_df: 35\n",
      "  seller_perform_df: 26\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 35 rows\n",
      "After joining items_df: 35 rows\n",
      "After joining logistic_df: 35 rows\n",
      "After joining shipping_df: 35 rows\n",
      "After joining history_df: 35 rows\n",
      "After joining seller_perform_df: 35 rows\n",
      "After adding date columns: 35 rows\n",
      "After joining concentration_df: 35 rows\n",
      "After dropping unused columns: 35 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 35\n",
      "saved to: datamart/gold/feature_store/2017_01_29.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-01-29\n",
      "Number of rows in feature store: 35\n",
      "Gold feature tables built successfully from start date: 2017-01-29\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-30\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:34:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_30.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_01_30.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_30.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_01_30.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_01_30.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_01_30.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-01-30\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 51\n",
      "  items_df: 112650\n",
      "  logistic_df: 51\n",
      "  shipping_df: 51\n",
      "  history_df: 51\n",
      "  seller_perform_df: 36\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 51 rows\n",
      "After joining items_df: 51 rows\n",
      "After joining logistic_df: 51 rows\n",
      "After joining shipping_df: 51 rows\n",
      "After joining history_df: 51 rows\n",
      "After joining seller_perform_df: 51 rows\n",
      "After adding date columns: 51 rows\n",
      "After joining concentration_df: 51 rows\n",
      "After dropping unused columns: 51 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 51\n",
      "saved to: datamart/gold/feature_store/2017_01_30.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-01-30\n",
      "Number of rows in feature store: 51\n",
      "Gold feature tables built successfully from start date: 2017-01-30\n",
      "execute command: python gold_feature_store.py --startdate \"2017-01-31\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:35:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_31.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_01_31.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_01_31.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_01_31.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_01_31.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_01_31.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-01-31\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 67\n",
      "  items_df: 112650\n",
      "  logistic_df: 67\n",
      "  shipping_df: 67\n",
      "  history_df: 67\n",
      "  seller_perform_df: 50\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 67 rows\n",
      "After joining items_df: 67 rows\n",
      "After joining logistic_df: 67 rows\n",
      "After joining shipping_df: 67 rows\n",
      "After joining history_df: 67 rows\n",
      "After joining seller_perform_df: 67 rows\n",
      "After adding date columns: 67 rows\n",
      "After joining concentration_df: 67 rows\n",
      "After dropping unused columns: 67 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 67\n",
      "saved to: datamart/gold/feature_store/2017_01_31.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-01-31\n",
      "Number of rows in feature store: 67\n",
      "Gold feature tables built successfully from start date: 2017-01-31\n",
      "execute command: python gold_feature_store.py --startdate \"2017-02-01\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:35:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_01.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_02_01.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_01.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_02_01.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_02_01.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_02_01.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-02-01\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 67\n",
      "  items_df: 112650\n",
      "  logistic_df: 67\n",
      "  shipping_df: 67\n",
      "  history_df: 67\n",
      "  seller_perform_df: 53\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 67 rows\n",
      "After joining items_df: 67 rows\n",
      "After joining logistic_df: 67 rows\n",
      "After joining shipping_df: 67 rows\n",
      "After joining history_df: 67 rows\n",
      "After joining seller_perform_df: 67 rows\n",
      "After adding date columns: 67 rows\n",
      "After joining concentration_df: 67 rows\n",
      "After dropping unused columns: 67 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 67\n",
      "saved to: datamart/gold/feature_store/2017_02_01.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-02-01\n",
      "Number of rows in feature store: 67\n",
      "Gold feature tables built successfully from start date: 2017-02-01\n",
      "execute command: python gold_feature_store.py --startdate \"2017-02-02\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:35:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_02.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_02_02.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_02.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_02_02.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_02_02.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_02_02.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-02-02\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 68\n",
      "  items_df: 112650\n",
      "  logistic_df: 68\n",
      "  shipping_df: 68\n",
      "  history_df: 68\n",
      "  seller_perform_df: 50\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 68 rows\n",
      "After joining items_df: 68 rows\n",
      "After joining logistic_df: 68 rows\n",
      "After joining shipping_df: 68 rows\n",
      "After joining history_df: 68 rows\n",
      "After joining seller_perform_df: 68 rows\n",
      "After adding date columns: 68 rows\n",
      "After joining concentration_df: 68 rows\n",
      "After dropping unused columns: 68 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 68\n",
      "saved to: datamart/gold/feature_store/2017_02_02.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-02-02\n",
      "Number of rows in feature store: 68\n",
      "Gold feature tables built successfully from start date: 2017-02-02\n",
      "execute command: python gold_feature_store.py --startdate \"2017-02-03\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:35:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_03.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_02_03.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_03.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_02_03.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_02_03.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_02_03.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-02-03\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 57\n",
      "  items_df: 112650\n",
      "  logistic_df: 57\n",
      "  shipping_df: 57\n",
      "  history_df: 57\n",
      "  seller_perform_df: 40\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 57 rows\n",
      "After joining items_df: 57 rows\n",
      "After joining logistic_df: 57 rows\n",
      "After joining shipping_df: 57 rows\n",
      "After joining history_df: 57 rows\n",
      "After joining seller_perform_df: 57 rows\n",
      "After adding date columns: 57 rows\n",
      "After joining concentration_df: 57 rows\n",
      "After dropping unused columns: 57 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 57\n",
      "saved to: datamart/gold/feature_store/2017_02_03.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-02-03\n",
      "Number of rows in feature store: 57\n",
      "Gold feature tables built successfully from start date: 2017-02-03\n",
      "execute command: python gold_feature_store.py --startdate \"2017-02-04\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:36:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_04.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_02_04.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_04.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_02_04.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_02_04.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_02_04.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-02-04\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 66\n",
      "  items_df: 112650\n",
      "  logistic_df: 66\n",
      "  shipping_df: 66\n",
      "  history_df: 66\n",
      "  seller_perform_df: 51\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 66 rows\n",
      "After joining items_df: 66 rows\n",
      "After joining logistic_df: 66 rows\n",
      "After joining shipping_df: 66 rows\n",
      "After joining history_df: 66 rows\n",
      "After joining seller_perform_df: 66 rows\n",
      "After adding date columns: 66 rows\n",
      "After joining concentration_df: 66 rows\n",
      "After dropping unused columns: 66 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 66\n",
      "saved to: datamart/gold/feature_store/2017_02_04.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-02-04\n",
      "Number of rows in feature store: 66\n",
      "Gold feature tables built successfully from start date: 2017-02-04\n",
      "execute command: python gold_feature_store.py --startdate \"2017-02-05\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:36:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_05.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_02_05.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_05.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_02_05.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_02_05.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_02_05.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-02-05\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 75\n",
      "  items_df: 112650\n",
      "  logistic_df: 75\n",
      "  shipping_df: 75\n",
      "  history_df: 75\n",
      "  seller_perform_df: 52\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 75 rows\n",
      "After joining items_df: 75 rows\n",
      "After joining logistic_df: 75 rows\n",
      "After joining shipping_df: 75 rows\n",
      "After joining history_df: 75 rows\n",
      "After joining seller_perform_df: 75 rows\n",
      "After adding date columns: 75 rows\n",
      "After joining concentration_df: 75 rows\n",
      "After dropping unused columns: 75 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 75\n",
      "saved to: datamart/gold/feature_store/2017_02_05.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-02-05\n",
      "Number of rows in feature store: 75\n",
      "Gold feature tables built successfully from start date: 2017-02-05\n",
      "execute command: python gold_feature_store.py --startdate \"2017-02-06\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:36:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_06.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_02_06.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_06.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_02_06.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_02_06.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_02_06.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-02-06\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 81\n",
      "  items_df: 112650\n",
      "  logistic_df: 81\n",
      "  shipping_df: 81\n",
      "  history_df: 81\n",
      "  seller_perform_df: 61\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 81 rows\n",
      "After joining items_df: 81 rows\n",
      "After joining logistic_df: 81 rows\n",
      "After joining shipping_df: 81 rows\n",
      "After joining history_df: 81 rows\n",
      "After joining seller_perform_df: 81 rows\n",
      "After adding date columns: 81 rows\n",
      "After joining concentration_df: 81 rows\n",
      "After dropping unused columns: 81 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 81\n",
      "saved to: datamart/gold/feature_store/2017_02_06.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-02-06\n",
      "Number of rows in feature store: 81\n",
      "Gold feature tables built successfully from start date: 2017-02-06\n",
      "execute command: python gold_feature_store.py --startdate \"2017-02-07\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:37:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_07.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_02_07.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_07.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_02_07.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_02_07.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_02_07.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-02-07\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 111\n",
      "  items_df: 112650\n",
      "  logistic_df: 111\n",
      "  shipping_df: 111\n",
      "  history_df: 111\n",
      "  seller_perform_df: 78\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 111 rows\n",
      "After joining items_df: 111 rows\n",
      "After joining logistic_df: 111 rows\n",
      "After joining shipping_df: 111 rows\n",
      "After joining history_df: 111 rows\n",
      "After joining seller_perform_df: 111 rows\n",
      "After adding date columns: 111 rows\n",
      "After joining concentration_df: 111 rows\n",
      "After dropping unused columns: 111 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 111\n",
      "saved to: datamart/gold/feature_store/2017_02_07.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-02-07\n",
      "Number of rows in feature store: 111\n",
      "Gold feature tables built successfully from start date: 2017-02-07\n",
      "execute command: python gold_feature_store.py --startdate \"2017-02-08\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:37:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_08.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_02_08.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_08.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_02_08.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_02_08.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_02_08.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-02-08\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 90\n",
      "  items_df: 112650\n",
      "  logistic_df: 90\n",
      "  shipping_df: 90\n",
      "  history_df: 90\n",
      "  seller_perform_df: 69\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 90 rows\n",
      "After joining items_df: 90 rows\n",
      "After joining logistic_df: 90 rows\n",
      "After joining shipping_df: 90 rows\n",
      "After joining history_df: 90 rows\n",
      "After joining seller_perform_df: 90 rows\n",
      "After adding date columns: 90 rows\n",
      "After joining concentration_df: 90 rows\n",
      "After dropping unused columns: 90 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 90\n",
      "saved to: datamart/gold/feature_store/2017_02_08.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-02-08\n",
      "Number of rows in feature store: 90\n",
      "Gold feature tables built successfully from start date: 2017-02-08\n",
      "execute command: python gold_feature_store.py --startdate \"2017-02-09\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:37:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_09.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_02_09.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_09.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_02_09.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_02_09.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_02_09.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-02-09\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 77\n",
      "  items_df: 112650\n",
      "  logistic_df: 77\n",
      "  shipping_df: 77\n",
      "  history_df: 77\n",
      "  seller_perform_df: 61\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 77 rows\n",
      "After joining items_df: 77 rows\n",
      "After joining logistic_df: 77 rows\n",
      "After joining shipping_df: 77 rows\n",
      "After joining history_df: 77 rows\n",
      "After joining seller_perform_df: 77 rows\n",
      "After adding date columns: 77 rows\n",
      "After joining concentration_df: 77 rows\n",
      "After dropping unused columns: 77 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 77\n",
      "saved to: datamart/gold/feature_store/2017_02_09.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-02-09\n",
      "Number of rows in feature store: 77\n",
      "Gold feature tables built successfully from start date: 2017-02-09\n",
      "execute command: python gold_feature_store.py --startdate \"2017-02-10\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:37:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_10.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_02_10.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_10.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_02_10.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_02_10.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_02_10.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-02-10\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 66\n",
      "  items_df: 112650\n",
      "  logistic_df: 66\n",
      "  shipping_df: 66\n",
      "  history_df: 66\n",
      "  seller_perform_df: 54\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 66 rows\n",
      "After joining items_df: 66 rows\n",
      "After joining logistic_df: 66 rows\n",
      "After joining shipping_df: 66 rows\n",
      "After joining history_df: 66 rows\n",
      "After joining seller_perform_df: 66 rows\n",
      "After adding date columns: 66 rows\n",
      "After joining concentration_df: 66 rows\n",
      "After dropping unused columns: 66 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 66\n",
      "saved to: datamart/gold/feature_store/2017_02_10.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-02-10\n",
      "Number of rows in feature store: 66\n",
      "Gold feature tables built successfully from start date: 2017-02-10\n",
      "execute command: python gold_feature_store.py --startdate \"2017-02-11\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:38:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_11.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_02_11.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_11.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_02_11.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_02_11.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_02_11.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-02-11\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 45\n",
      "  items_df: 112650\n",
      "  logistic_df: 45\n",
      "  shipping_df: 45\n",
      "  history_df: 45\n",
      "  seller_perform_df: 42\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 45 rows\n",
      "After joining items_df: 45 rows\n",
      "After joining logistic_df: 45 rows\n",
      "After joining shipping_df: 45 rows\n",
      "After joining history_df: 45 rows\n",
      "After joining seller_perform_df: 45 rows\n",
      "After adding date columns: 45 rows\n",
      "After joining concentration_df: 45 rows\n",
      "After dropping unused columns: 45 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 45\n",
      "saved to: datamart/gold/feature_store/2017_02_11.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-02-11\n",
      "Number of rows in feature store: 45\n",
      "Gold feature tables built successfully from start date: 2017-02-11\n",
      "execute command: python gold_feature_store.py --startdate \"2017-02-12\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:38:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_12.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_02_12.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_12.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_02_12.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_02_12.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_02_12.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-02-12\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 63\n",
      "  items_df: 112650\n",
      "  logistic_df: 63\n",
      "  shipping_df: 63\n",
      "  history_df: 63\n",
      "  seller_perform_df: 52\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 63 rows\n",
      "After joining items_df: 63 rows\n",
      "After joining logistic_df: 63 rows\n",
      "After joining shipping_df: 63 rows\n",
      "After joining history_df: 63 rows\n",
      "After joining seller_perform_df: 63 rows\n",
      "After adding date columns: 63 rows\n",
      "After joining concentration_df: 63 rows\n",
      "After dropping unused columns: 63 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 63\n",
      "saved to: datamart/gold/feature_store/2017_02_12.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-02-12\n",
      "Number of rows in feature store: 63\n",
      "Gold feature tables built successfully from start date: 2017-02-12\n",
      "execute command: python gold_feature_store.py --startdate \"2017-02-13\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:38:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_13.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_02_13.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_13.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_02_13.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_02_13.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_02_13.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-02-13\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 76\n",
      "  items_df: 112650\n",
      "  logistic_df: 76\n",
      "  shipping_df: 76\n",
      "  history_df: 76\n",
      "  seller_perform_df: 58\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 76 rows\n",
      "After joining items_df: 76 rows\n",
      "After joining logistic_df: 76 rows\n",
      "After joining shipping_df: 76 rows\n",
      "After joining history_df: 76 rows\n",
      "After joining seller_perform_df: 76 rows\n",
      "After adding date columns: 76 rows\n",
      "After joining concentration_df: 76 rows\n",
      "After dropping unused columns: 76 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 76\n",
      "saved to: datamart/gold/feature_store/2017_02_13.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-02-13\n",
      "Number of rows in feature store: 76\n",
      "Gold feature tables built successfully from start date: 2017-02-13\n",
      "execute command: python gold_feature_store.py --startdate \"2017-02-14\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:38:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_14.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_02_14.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_14.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_02_14.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_02_14.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_02_14.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-02-14\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 71\n",
      "  items_df: 112650\n",
      "  logistic_df: 71\n",
      "  shipping_df: 71\n",
      "  history_df: 71\n",
      "  seller_perform_df: 62\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 71 rows\n",
      "After joining items_df: 71 rows\n",
      "After joining logistic_df: 71 rows\n",
      "After joining shipping_df: 71 rows\n",
      "After joining history_df: 71 rows\n",
      "After joining seller_perform_df: 71 rows\n",
      "After adding date columns: 71 rows\n",
      "After joining concentration_df: 71 rows\n",
      "After dropping unused columns: 71 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 71\n",
      "saved to: datamart/gold/feature_store/2017_02_14.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-02-14\n",
      "Number of rows in feature store: 71\n",
      "Gold feature tables built successfully from start date: 2017-02-14\n",
      "execute command: python gold_feature_store.py --startdate \"2017-02-15\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:39:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_15.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_02_15.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_15.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_02_15.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_02_15.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_02_15.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-02-15\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 60\n",
      "  items_df: 112650\n",
      "  logistic_df: 60\n",
      "  shipping_df: 60\n",
      "  history_df: 60\n",
      "  seller_perform_df: 51\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 60 rows\n",
      "After joining items_df: 60 rows\n",
      "After joining logistic_df: 60 rows\n",
      "After joining shipping_df: 60 rows\n",
      "After joining history_df: 60 rows\n",
      "After joining seller_perform_df: 60 rows\n",
      "After adding date columns: 60 rows\n",
      "After joining concentration_df: 60 rows\n",
      "After dropping unused columns: 60 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 60\n",
      "saved to: datamart/gold/feature_store/2017_02_15.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-02-15\n",
      "Number of rows in feature store: 60\n",
      "Gold feature tables built successfully from start date: 2017-02-15\n",
      "execute command: python gold_feature_store.py --startdate \"2017-02-16\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:39:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_16.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_02_16.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_16.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_02_16.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_02_16.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_02_16.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-02-16\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 52\n",
      "  items_df: 112650\n",
      "  logistic_df: 52\n",
      "  shipping_df: 52\n",
      "  history_df: 52\n",
      "  seller_perform_df: 45\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 52 rows\n",
      "After joining items_df: 52 rows\n",
      "After joining logistic_df: 52 rows\n",
      "After joining shipping_df: 52 rows\n",
      "After joining history_df: 52 rows\n",
      "After joining seller_perform_df: 52 rows\n",
      "After adding date columns: 52 rows\n",
      "After joining concentration_df: 52 rows\n",
      "After dropping unused columns: 52 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 52\n",
      "saved to: datamart/gold/feature_store/2017_02_16.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-02-16\n",
      "Number of rows in feature store: 52\n",
      "Gold feature tables built successfully from start date: 2017-02-16\n",
      "execute command: python gold_feature_store.py --startdate \"2017-02-17\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:39:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_17.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_02_17.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_17.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_02_17.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_02_17.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_02_17.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-02-17\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 45\n",
      "  items_df: 112650\n",
      "  logistic_df: 45\n",
      "  shipping_df: 45\n",
      "  history_df: 45\n",
      "  seller_perform_df: 40\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 45 rows\n",
      "After joining items_df: 45 rows\n",
      "After joining logistic_df: 45 rows\n",
      "After joining shipping_df: 45 rows\n",
      "After joining history_df: 45 rows\n",
      "After joining seller_perform_df: 45 rows\n",
      "After adding date columns: 45 rows\n",
      "After joining concentration_df: 45 rows\n",
      "After dropping unused columns: 45 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 45\n",
      "saved to: datamart/gold/feature_store/2017_02_17.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-02-17\n",
      "Number of rows in feature store: 45\n",
      "Gold feature tables built successfully from start date: 2017-02-17\n",
      "execute command: python gold_feature_store.py --startdate \"2017-02-18\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:40:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Gold root directory: datamart/gold\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_18.parquet']\n",
      "Reading 1 file(s) for table 'order_items': ['silver_olist_order_items.parquet']\n",
      "Reading 1 file(s) for table 'order_logistics': ['silver_olist_order_logistics_2017_02_18.parquet']\n",
      "Reading 1 file(s) for table 'orders': ['silver_olist_orders_2017_02_18.parquet']\n",
      "Reading 1 file(s) for table 'shipping_infos': ['silver_shipping_infos_2017_02_18.parquet']\n",
      "Reading 1 file(s) for table 'delivery_history': ['silver_delivery_history_2017_02_18.parquet']\n",
      "Reading 1 file(s) for table 'seller_performance': ['silver_seller_performance_2017_02_18.parquet']\n",
      "Reading 1 file(s) for table 'concentration': ['silver_concentration_2016_09_04.parquet']\n",
      "Processing feature gold table for snapshot date: 2017-02-18\n",
      "Input DataFrame row counts:\n",
      "  orders_df: 52\n",
      "  items_df: 112650\n",
      "  logistic_df: 52\n",
      "  shipping_df: 52\n",
      "  history_df: 52\n",
      "  seller_perform_df: 38\n",
      "  concentration_df: 4\n",
      "After selecting order columns: 52 rows\n",
      "After joining items_df: 52 rows\n",
      "After joining logistic_df: 52 rows\n",
      "After joining shipping_df: 52 rows\n",
      "After joining history_df: 52 rows\n",
      "After joining seller_perform_df: 52 rows\n",
      "After adding date columns: 52 rows\n",
      "After joining concentration_df: 52 rows\n",
      "After dropping unused columns: 52 rows\n",
      "Final DataFrame columns: ['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price', 'total_freight_value', 'total_weight_g', 'total_volume_cm3', 'total_density', 'seller_city', 'seller_state', 'delivery_distance', 'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver', 'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week', 'season', 'concentration']\n",
      "Final DataFrame row count: 52\n",
      "saved to: datamart/gold/feature_store/2017_02_18.parquet\n",
      "Feature gold table processing completed for snapshot date: 2017-02-18\n",
      "Number of rows in feature store: 52\n",
      "Gold feature tables built successfully from start date: 2017-02-18\n",
      "execute command: python gold_feature_store.py --startdate \"2017-02-19\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 06:40:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "start_date = datetime.date(2016, 9, 4)\n",
    "end_date = datetime.date(2018, 10, 17)\n",
    "\n",
    "for i in range((end_date - start_date).days + 1):\n",
    "    day = start_date + datetime.timedelta(days=i)\n",
    "    day_str = day.strftime(\"%Y-%m-%d\")\n",
    "    cmd = f'python gold_feature_store.py --startdate \"{day_str}\"'\n",
    "    print(f'execute command: {cmd}')\n",
    "    os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0040c2eb-88f3-4906-9ddd-339fbffc9de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-20.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Downloading pyarrow-20.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-20.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fed17eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8969fb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('datamart/gold/feature_store/2016_09_15.parquet/', engine=\"pyarrow\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9efc595-dbb3-4f8d-b79f-fd4fc430274b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['snapshot_date', 'order_id', 'order_status', 'total_qty', 'total_price',\n",
       "       'total_freight_value', 'total_weight_g', 'total_volume_cm3',\n",
       "       'total_density', 'seller_city', 'seller_state', 'delivery_distance',\n",
       "       'same_city', 'same_state', 'is_weekend', 'act_days_to_deliver',\n",
       "       'avg_rating', 'avg_delay_rate', 'avg_processing_time', 'day_of_week',\n",
       "       'season', 'concentration'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "955b12d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>total_qty</th>\n",
       "      <th>total_price</th>\n",
       "      <th>total_freight_value</th>\n",
       "      <th>total_weight_g</th>\n",
       "      <th>total_volume_cm3</th>\n",
       "      <th>total_density</th>\n",
       "      <th>main_category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>snapshot_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2e7a8482f6fb09756ca50c10d7bfc047</td>\n",
       "      <td>2016-09-04 21:15:19</td>\n",
       "      <td>2</td>\n",
       "      <td>72.89</td>\n",
       "      <td>63.34</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>10752.0</td>\n",
       "      <td>0.297619</td>\n",
       "      <td>furniture</td>\n",
       "      <td>décor</td>\n",
       "      <td>2016-09-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           order_id order_purchase_timestamp  total_qty  \\\n",
       "0  2e7a8482f6fb09756ca50c10d7bfc047      2016-09-04 21:15:19          2   \n",
       "\n",
       "   total_price  total_freight_value  total_weight_g  total_volume_cm3  \\\n",
       "0        72.89                63.34          3200.0           10752.0   \n",
       "\n",
       "   total_density main_category sub_category snapshot_date  \n",
       "0       0.297619     furniture        décor    2016-09-04  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logistics\n",
    "df = pd.read_parquet('datamart/silver/order_logistics/silver_olist_order_logistics_04_09_2016.parquet', engine=\"pyarrow\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0a966b1-7591-4a7e-a180-360e00244f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['order_id', 'order_purchase_timestamp', 'total_qty', 'total_price',\n",
       "       'total_freight_value', 'total_weight_g', 'total_volume_cm3',\n",
       "       'total_density', 'main_category', 'sub_category', 'snapshot_date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08a192aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_status</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>order_approved_at</th>\n",
       "      <th>order_delivered_carrier_date</th>\n",
       "      <th>order_delivered_customer_date</th>\n",
       "      <th>order_estimated_delivery_date</th>\n",
       "      <th>snapshot_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2e7a8482f6fb09756ca50c10d7bfc047</td>\n",
       "      <td>08c5351a6aca1c1589a38f244edeee9d</td>\n",
       "      <td>shipped</td>\n",
       "      <td>2016-09-04 21:15:19</td>\n",
       "      <td>2016-10-07 13:18:03</td>\n",
       "      <td>2016-10-18 13:14:51</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2016-10-20</td>\n",
       "      <td>2016-09-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           order_id                       customer_id  \\\n",
       "0  2e7a8482f6fb09756ca50c10d7bfc047  08c5351a6aca1c1589a38f244edeee9d   \n",
       "\n",
       "  order_status order_purchase_timestamp   order_approved_at  \\\n",
       "0      shipped      2016-09-04 21:15:19 2016-10-07 13:18:03   \n",
       "\n",
       "  order_delivered_carrier_date order_delivered_customer_date  \\\n",
       "0          2016-10-18 13:14:51                           NaT   \n",
       "\n",
       "  order_estimated_delivery_date snapshot_date  \n",
       "0                    2016-10-20    2016-09-04  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# order\n",
    "df = pd.read_parquet('datamart/silver/orders/silver_olist_orders_04_09_2016.parquet', engine=\"pyarrow\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d30b2cbe-17bd-4f5f-b511-34a0a6a215a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['order_id', 'customer_id', 'order_status', 'order_purchase_timestamp',\n",
       "       'order_approved_at', 'order_delivered_carrier_date',\n",
       "       'order_delivered_customer_date', 'order_estimated_delivery_date',\n",
       "       'snapshot_date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b32cbc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>order_item_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>shipping_limit_date</th>\n",
       "      <th>price</th>\n",
       "      <th>freight_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89bed55dd88d035e3b1b2489862e53cc</td>\n",
       "      <td>1</td>\n",
       "      <td>ae6b739ab6e9d7991fb2ddd70f9c0b6b</td>\n",
       "      <td>53e4c6e0f4312d4d2107a8c9cddf45cd</td>\n",
       "      <td>2018-01-11 17:12:22</td>\n",
       "      <td>42.00</td>\n",
       "      <td>17.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89c037e2b749a2ed5e9e1219189e241e</td>\n",
       "      <td>1</td>\n",
       "      <td>e906fa76a27488f805ba8584a4c10cba</td>\n",
       "      <td>1835b56ce799e6a4dc4eddc053f04066</td>\n",
       "      <td>2017-11-30 04:15:33</td>\n",
       "      <td>53.99</td>\n",
       "      <td>12.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89c037e2b749a2ed5e9e1219189e241e</td>\n",
       "      <td>2</td>\n",
       "      <td>e906fa76a27488f805ba8584a4c10cba</td>\n",
       "      <td>1835b56ce799e6a4dc4eddc053f04066</td>\n",
       "      <td>2017-11-30 04:15:33</td>\n",
       "      <td>53.99</td>\n",
       "      <td>12.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89c04d2250464948257c7dbd4e41c3cc</td>\n",
       "      <td>1</td>\n",
       "      <td>3d73c88390adac7dd50fffe6c00f1022</td>\n",
       "      <td>c8b0e2b0a7095e5d8219575d5e7e1181</td>\n",
       "      <td>2018-03-14 00:20:26</td>\n",
       "      <td>199.99</td>\n",
       "      <td>25.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89c0bf5292a493fb2a7aacae2148eeac</td>\n",
       "      <td>1</td>\n",
       "      <td>2d40d83fc97b8d4d468b4ab5a6a0810d</td>\n",
       "      <td>cca3071e3e9bb7d12640c9fbe2301306</td>\n",
       "      <td>2017-08-24 03:26:15</td>\n",
       "      <td>29.90</td>\n",
       "      <td>11.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           order_id  order_item_id  \\\n",
       "0  89bed55dd88d035e3b1b2489862e53cc              1   \n",
       "1  89c037e2b749a2ed5e9e1219189e241e              1   \n",
       "2  89c037e2b749a2ed5e9e1219189e241e              2   \n",
       "3  89c04d2250464948257c7dbd4e41c3cc              1   \n",
       "4  89c0bf5292a493fb2a7aacae2148eeac              1   \n",
       "\n",
       "                         product_id                         seller_id  \\\n",
       "0  ae6b739ab6e9d7991fb2ddd70f9c0b6b  53e4c6e0f4312d4d2107a8c9cddf45cd   \n",
       "1  e906fa76a27488f805ba8584a4c10cba  1835b56ce799e6a4dc4eddc053f04066   \n",
       "2  e906fa76a27488f805ba8584a4c10cba  1835b56ce799e6a4dc4eddc053f04066   \n",
       "3  3d73c88390adac7dd50fffe6c00f1022  c8b0e2b0a7095e5d8219575d5e7e1181   \n",
       "4  2d40d83fc97b8d4d468b4ab5a6a0810d  cca3071e3e9bb7d12640c9fbe2301306   \n",
       "\n",
       "  shipping_limit_date   price  freight_value  \n",
       "0 2018-01-11 17:12:22   42.00          17.60  \n",
       "1 2017-11-30 04:15:33   53.99          12.72  \n",
       "2 2017-11-30 04:15:33   53.99          12.72  \n",
       "3 2018-03-14 00:20:26  199.99          25.05  \n",
       "4 2017-08-24 03:26:15   29.90          11.85  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# items\n",
    "df = pd.read_parquet('datamart/silver/order_items/silver_olist_order_items.parquet', engine=\"pyarrow\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0995d8e3-8468-4558-90cc-2b7edeaa6334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['order_id', 'order_item_id', 'product_id', 'seller_id',\n",
       "       'shipping_limit_date', 'price', 'freight_value'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf2d1a30-571d-40b5-8b54-2c84f42b2cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>customer_zip_code_prefix</th>\n",
       "      <th>customer_city</th>\n",
       "      <th>customer_state</th>\n",
       "      <th>customer_lat</th>\n",
       "      <th>customer_lng</th>\n",
       "      <th>seller_zip_code_prefix</th>\n",
       "      <th>seller_city</th>\n",
       "      <th>seller_state</th>\n",
       "      <th>seller_lat</th>\n",
       "      <th>seller_lng</th>\n",
       "      <th>delivery_distance</th>\n",
       "      <th>same_zipcode</th>\n",
       "      <th>same_city</th>\n",
       "      <th>same_state</th>\n",
       "      <th>snapshot_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2e7a8482f6fb09756ca50c10d7bfc047</td>\n",
       "      <td>2016-09-04 21:15:19</td>\n",
       "      <td>69309</td>\n",
       "      <td>boa vista</td>\n",
       "      <td>RR</td>\n",
       "      <td>2.813746</td>\n",
       "      <td>-60.701007</td>\n",
       "      <td>37580</td>\n",
       "      <td>monte siao</td>\n",
       "      <td>MG</td>\n",
       "      <td>-22.430218</td>\n",
       "      <td>-46.573405</td>\n",
       "      <td>3198.843108</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           order_id order_purchase_timestamp  \\\n",
       "0  2e7a8482f6fb09756ca50c10d7bfc047      2016-09-04 21:15:19   \n",
       "\n",
       "  customer_zip_code_prefix customer_city customer_state  customer_lat  \\\n",
       "0                    69309     boa vista             RR      2.813746   \n",
       "\n",
       "   customer_lng seller_zip_code_prefix seller_city seller_state  seller_lat  \\\n",
       "0    -60.701007                  37580  monte siao           MG  -22.430218   \n",
       "\n",
       "   seller_lng  delivery_distance  same_zipcode  same_city  same_state  \\\n",
       "0  -46.573405        3198.843108             0          0           0   \n",
       "\n",
       "  snapshot_date  \n",
       "0    2016-09-04  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shipping_info\n",
    "df = pd.read_parquet('datamart/silver/shipping_infos/silver_shipping_infos_04_09_2016.parquet', engine=\"pyarrow\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0ae0c41-44c4-49e0-9000-8b3e83a18993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['order_id', 'order_purchase_timestamp', 'customer_zip_code_prefix',\n",
       "       'customer_city', 'customer_state', 'customer_lat', 'customer_lng',\n",
       "       'seller_zip_code_prefix', 'seller_city', 'seller_state', 'seller_lat',\n",
       "       'seller_lng', 'delivery_distance', 'same_zipcode', 'same_city',\n",
       "       'same_state', 'snapshot_date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96136abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snapshot_date</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>avg_delay_rate</th>\n",
       "      <th>avg_processing_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-09-04</td>\n",
       "      <td>1554a68530182680ad5c8b042c3ab563</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  snapshot_date                         seller_id  avg_rating  avg_delay_rate  \\\n",
       "0    2016-09-04  1554a68530182680ad5c8b042c3ab563         1.0             NaN   \n",
       "\n",
       "   avg_processing_time  \n",
       "0                  NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seller_performance\n",
    "df = pd.read_parquet('datamart/silver/seller_performance/silver_seller_performance_04_09_2016.parquet', engine=\"pyarrow\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26c91531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snapshot_date</th>\n",
       "      <th>granularity_level</th>\n",
       "      <th>type</th>\n",
       "      <th>region</th>\n",
       "      <th>concentration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-09-04</td>\n",
       "      <td>city</td>\n",
       "      <td>customer</td>\n",
       "      <td>boa vista</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-09-04</td>\n",
       "      <td>state</td>\n",
       "      <td>customer</td>\n",
       "      <td>RR</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-09-04</td>\n",
       "      <td>city</td>\n",
       "      <td>seller</td>\n",
       "      <td>monte siao</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-09-04</td>\n",
       "      <td>state</td>\n",
       "      <td>seller</td>\n",
       "      <td>MG</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  snapshot_date granularity_level      type      region  concentration\n",
       "0    2016-09-04              city  customer   boa vista          100.0\n",
       "1    2016-09-04             state  customer          RR          100.0\n",
       "2    2016-09-04              city    seller  monte siao          100.0\n",
       "3    2016-09-04             state    seller          MG          100.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concentration\n",
    "df = pd.read_parquet('datamart/silver/concentration/silver_concentration_04_09_2016.parquet', engine=\"pyarrow\")\n",
    "df.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
